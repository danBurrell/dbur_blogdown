---
title: A single factor CRD and the one-way analysis of variance
author: Daniel Burrell
date: '2020-04-03'
slug: a-single-factor-crd-and-the-one-way-analysis-of-variance
categories:
  - R
  - analysis of variance
  - design of experiments
  - completely randomised design
tags:
  - CRD
  - ANOVA
  - DOE
description: ''
topics: []
---

I'm currently teaching a class on the statistical design of experiments, and we're using SPSS as the statistical software to implement basic analyses of data from designed experiments at the moment. Shortly we'd like to switch to using R as the main statistical computing package. I also have a client who has asked me to teach her how to use R to perform basic one-way and two-way analysis of variance for data arising from completely randomised designs with single factor and 2-factorial treatment structures, respectively. This is child's play, especially in R, but I thought I'd take the opportunity to work through a simple example for future use.

To give credit where it is due, I have borrowed from Montgomery (1991) _Design and Analysis of Experiments_. Oehlert (2010) [_A First Course in Design and Analysis of Experiments_](http://users.stat.umn.edu/~gary/book/fcdae.pdf), Meier (2018) [_ANOVA: A Short Intro Using R_](https://stat.ethz.ch/~meier/teaching/anova/) and an online tutorial introduction to one-way ANOVA by [STHDA](http://www.sthda.com/english/wiki/one-way-anova-test-in-r) to develop this post.  

## What is the one-way ANOVA test

The __one-way analysis of variance (ANOVA)__ extends the two independent samples t-test to the case where there are more than two independent samples or groups to be compared. In a __one-way ANOVA__ the data is organised into groups based on a single grouping variable, which is called, in the language of experimental design, a __factor__. For this reason the one-way ANOVA is sometimes referred to as a _one-factor_ or _single-factor ANOVA_. The groups usually correspond to different treatment conditions. The single factor is often referred to as the __treatment__ or __treatment factor__ and the different treatment conditions are referred to as its __levels__. 

Let's introduce some notation. Suppose we have a treatment factor with $T$ levels corresponding to $T$ distinct treatment conditions applied to $N$ experimental units in a completely randomised fashion such that each of the treatment conditions gets assigned to $r_i$ experimental units, where 
\[
\sum_{i=1}^T r_i = N.
\]
The value $r_j$ is the sample size of the $j$th treatment group, and is called it number of replicates. The optimal choice (with respect to statistical power) of the $r_j$ for all $j$ actually depends on the research question, but for the case where $r_1 = r_2 = \cdots = r_T = r$ (i.e. each treatment group has an equal number of replicates, or all the treatment groups have the same sample size), the design is said to be __balanced__. 

The goal of the ANOVA is to compare the impacts of $T>2$ treatments on some response measurement. We do this by formulating a parametric model for our data. The classical model is called the __cell means model__.  Let $Y_{ij}$ denote the $j$th replicate observation in treatment group $i$, where $i = 1,\ldots,T$ and $j=1,\ldots, r_i$. In the __cell means model__ we allow each treatment group to have its own _expected value_ but we assume that the observations are independent and fluctuate around this value according to a normal distribution, i.e.,
\[
Y_{ij} \sim N(\mu_i, \sigma^2),\quad \text{independent}
\]
where

* $\mu_i = \mathbb E[Y_{ij}]$, the expected value of the response random variable for treatment group $i$
* $\sigma^2 = \mathbb V[Y_{ij}]$, the variance of response random variable, which is assumed to be homogenous or constant across all treatment groups.

The above can be re-written as
\[
Y_{ij} = \mu_i + \epsilon_{ij},
\]
with (random) _errors_ $\epsilon_{ij} \sim N(0, \sigma^2)$ by simply partitioning the $N(\mu_i, \sigma^2)$ distribution into a deterministic part $\mu_i$ and a stochastic part $\epsilon_{ij}$ fluctuating around _zero_. This clearly links to a simple linear regression, with $Y$ being the __response__ and the treatment factor encoding the grouping information being a categorical __predictor__. The one-way ANOVA is nothing more than a regression model with a categorical predictor and normally distributed errors. 

The categorical predictor or factor can be either unordered (__nominal__) or ordered (__ordinal__). For example, south-east Queensland wheat variety would be an unordered (nominal) factor  (e.g. with levels "Strzlecki", "EGA Wylie" and "Baxter"). There is no sense of ordering between these different varieties. Glyphosate concentration could be an ordered (ordinal) factor (e.g. with ordered levels "1 percent solution", "2 percent solution", "5 percent solution" and "10 percent solution"). There is clearly an order relation on the levels of Glyphosate concentration; we expect $1\% < 2\% < 5\% < 10\%$ in terms of the impact of the glyphosate as a herbicide.  

It's also possible to further re-write the deterministic part as
\[
\mu_i = \mu + \tau_i, \quad (i=1,\ldots,T)
\]
to obtain a model structure of the form
\[
Y_{ij} = \mu+\tau_i+\epsilon_{ij},
\]
again with $\epsilon_{ij} \sim N(0, \sigma^2)$

This is the __effects model__ where $\tau_i$ is the $i$th __treatment effect__. Think of $\mu$ as a "global mean" and $\tau_i$ as a "deviation from the global mean due to the effect of the $i$th treatment". Actually, this interpretation is not always correct, but it is helpful. 

Let's look more closely at the parameters of these two models (cell means and effects models). The cell means model requires us to estimate $\mu_1, \ldots,\mu_T$ and $\sigma^2$ for a total of $T+1$ parameters. But the effects model requires us to estimate $\mu$, $\tau_1,\ldots,\alpha_T$ and $\sigma^2$ for a total of $T+2$ parameters. The addition of the extra parameter in the effects model renders it __non-identifiable__ (because we have $T+2$ parameters and only $T+1$ independent bits of information with which to model them. In other words, we are free to "shift around" effects between $\mu$ and the $\alpha_i$'s without altering the resulting values of $\mu_i$. For example, we are free to add a constant to $\mu$, say $\mu + c$ and then adjust the $\tau_i$'s by subtracting the same constant, $\tau_i - c$, and this will lead to the same value of $\mu_i$ for each $i$. Because of this non-identifiability problem, we need to impose a constraint on the $\tau_i$'s that effectively "removes" the additional parameter and restores identifiability. Some of the commonly adopted constraints are:

* __sum-to-zero constraint:__ $\quad \sum_{i=1}^T \tau_i = 0$ leading to the interpretation that $\quad \mu = \frac{1}{T} \sum_{i=1}^T \mu_i$ (NB. In R this corresponds to the `contr.sum` method).
* __weighted sum-to-zero constraint:__ $\quad \sum_{i=1}^T r_i\tau_i = 0$ leading to the interpretation that $\mu = \frac{1}{N} \sum_{i=1}^T r_i\mu_i$.
* __reference group constraint:__ $\tau_1 = 0$ leading to the interpretation that $\mu = \mu_1$ (NB. In R this corresponds to the default `contr.treatment` method).

Only $T-1$ elements of the treatment effects are allowed to freely vary. If we know $T-1$ of the $\tau_i$ values, then we automatically know the remaining $\tau_i$ value. This fact is encoded in the treatment __degrees of freedom (df)__: we say that the treatment effects has $T-1$ degrees of freedom. 

Without going into full details, we may estimate the parameters of the model using the __least squares criterion__ which minimizes the squared deviation from the observed data $y_{ij}$ to the model values $\mu+\tau_i$, i.e., for the cell means model:
\[
\hat\mu_i = \arg\min_{\mu_i} \sum_{i=1}^T\sum_{j=1}^{r_i} (y_{ij} - \mu_i)^2,
\]
and for the effects model:
\[
\hat\mu,\hat\tau_i = \arg\min_{\mu,\tau_i} \sum_{i=1}^T\sum_{j=1}^{r_i} (y_{ij} - \mu - \tau_i)^2.
\]

We use the following notation:

* sum of group $i$: $\quad y_{i\cdot}=\sum_{j=1}^{r_i} y_{ij}$
* sum of all observations: $\quad y_{\cdot\cdot}=\sum_{i=1}^T\sum_{j=1}^{r_i} y_{ij}$
* mean of group $i$: $\quad \bar y_{i\cdot}=\frac{1}{r_i}\sum_{j=1}^{r_i} y_{ij}$
* overall (total/grand/global) mean: $\quad \bar y_{\cdot\cdot}=\frac{1}{N}\sum_{i=1}^T\sum_{j=1}^{r_i} y_{ij}$

As we can independently estimate the values of the different group means, we get that $\hat \mu_i = \bar y_{i\cdot}$, so that:
\[
\hat \mu_i = \hat \mu + \hat \tau_i = \bar y_i.
\]
Depending on the side-constraint that we use we get different results for $\hat\tau_i$.

We use the __mean square error__ $MS_E$ to estimate the error variance $\sigma^2$, giving:
\[
\hat\sigma^2 = MS_E = \frac{1}{N-T}SS_E,
\]
where $SS_E$ is the __residual__ or __error sum of squares__:
\[
SS_E = \sum_{i=1}^T\sum_{j=1}^{r_i} (y_{ij} - \hat \mu_i)^2.
\]
A little algebra illustrates the relationship between these values and the sample variance of the observations in each treatment group. We can write the mean square error as:
\[
MS_E = \frac{1}{N-T} \sum_{i=1}^T(r_i - 1)s_i^2,
\]
since $s_i^2$ is by definition:
\[
s_i^2 = \frac{1}{r_i - 1} \sum_{j=1}^{r_i} (y_{ij} - \hat\mu_i)^2.
\]
The denominator $N-T$ in $\hat\sigma^2 = MS_E$ ensures that as an estimator, it unbiasedly estimates the error variance $\sigma^2$.

Before we look at an example, let's first review in a less mathematical and more verbally descriptive form, the core details of the ANOVA.

## Assumptions of ANOVA test
A one-way ANOVA test can only be applied when the following assumptions are met:

* The observations are obtained independently and randomly from the population defined by the factor levels. That is, each factor level identifies an independent, random sample from the population of interest.
* The data of each factor level are normally distributed. That is, the populations from which the independent samples are randomly drawn under each treatment group are normally distributed. 
* These normal populations have a common variance. That is, despite the distributions of different treatment groups being allowed to have different means or expected values, the normal variance is meant to be constant across all treatment groups.

The third assumption is theoretically sensible when you think that in an experiment local control has been exercised and the assignment of treatments to experimental units has been at random. We are selecting from a larger population of _presumably_ relatively homogeous experimental units, and then the only thing that should be different about them, apart from inherent random variability (captured by the error variance), is whatever the applied treatments do to "shift" the treatment means away from the underlying overall mean. So, it's reasonable to expect that the variance of each treatment group is identical to the inherent random variability of the underlying population, namely the random error variance, since we only expect treatments to impact the normal mean and not its variance. Of course, the mechanism by which the treatment impacts the response may be less docile than our assumptions allow, and so in practice this assumption (and the others) may not hold. In terms of robustness, it turns out that the most important assumption is independence, followed by homogeneity of variance. The way in which we design and conduct the experiment is intended to force independence as much as possible. We can use __Levene's test__ and side-by-side boxplots to make judgements about the variability. Also, a general rule of thumb suggests that if the largest standard deviation of a treatment group is not more than twice as large as the smallest standard deviation of a treatment group, then we ought not be overly concerned about variance heterogeniety (Of course I must confess I haven't any real idea what the basis of this rule of thumb is, so I am hesitant to propagate it). To assess normality we can use kernel density plots and normal quantile-quantile plots id there is sufficient data. There are a variety of formal tests too. Common ones include the Kolmogorov-Smirnov test and the Shapiro-Wilks test. More will be said about testing the validity of the ANOVA assumptions in a later section.     

## How one-way ANOVA test works
If you're anything like me, you might still be wondering why on earth we use an analysis of _variance_ to compare _means_! 
Well, firstly, the hypotheses we seek to test using a one-way ANOVA F-test are as follows:

\[
\begin{aligned}
H_0 &: \mu_1 = \mu_2 = \cdots = \mu_T \\
H_1 &: \mu_k \neq \mu_l \quad \text{for at least one pair of treatment means}
\end{aligned}
\]

The ability to do this rests on the assumptions of homogeneity of variance in normal populations. Conditional on the null hypothesis being true (i.e. if it is true), we can come up with two distinct variance estimators that should both estimate the same population parameter, namely the error variance $\sigma^2$. We've already seen the first of these, the __mean square error $MS_E$__, which is the (weighted) average of the sample variances (i.e. the pooled sample variance). This measures the __variance within samples__ or the __variation that is inherent and due to random error__. But we can also derive an estimator of the __variance between samples__, a weighted measure of the variance of the sample means. This measures the __variation due to treatments or explained variation__. But under the null hypothesis, and crucially, if the assumption of homogeneity of variance is met, the distributions should be exactly the same, so these two estimators should have similar values (they're estimating the same underlying construct). We build these two estimators as follows. The $MS_E$ is built by calculating the $SS_E$ and dividing by the degrees of freedom for error, as shown earlier. In a similar fashion, we can compute a __treatment sum of squares $(SS_T)$__ and divide it by the degrees of freedom for treatments to get our variance estimator, the __mean square for treatments $(MS_T)$__. Informally, under the null hypothesis, we expect that these two independent estimators of the error variance should have a ratio of 1. That is, __if the null hypothesis is true__, we expect:
\[
\frac{MS_T}{MS_E}\approx 1
\]
But because we're assuming normality we can make this argument formal. Both these variance estimators can be algebraically re-arranged to enable us, via __Cochran's theorem__, to claim that they are random variables having independent (central) chi-square $\chi^2$ distributions with corresponding degrees of freedom (i.e. for $MS_T$ we have $\nu_1 = T-1$ and for $MS_E$ we have $\nu_2 = N-T$ degrees of freedom). This is good because the ratio of two independent (central) chi-square distributed random variables has an F distribution, so that the ratio of the variation due to treatments (between samples) to the variation due to error (within samples) should be F-distributed with $\nu_1$ numerator degrees of freedom and $\nu_2$ denominator degrees of freedom. 

In summary, to compare two or more treatment means using one-way ANOVA:

1. Compute the common variance, which is called the variance within samples, error variance or residual variance as $\hat\sigma^2_{E} = MS_E$ with $\nu_1 = N-T$ degrees of freedom.
2. Compute the variance between sample means as $\hat\sigma^2_{T}$ with $\nu_2 = T-1$ degrees of freedom. 
3. Produce the observed F-statistic as the ratio $\frac{\hat\sigma^2_{T}}{\hat\sigma^2_{E}}\sim F_{\nu_1,\nu_2}$. 

Then, informally, an observed F-statistic less than or equal to 1 indicates no significant difference between the treament means, but as the F-statistic increases beyond 1 the strength of evidence for a difference between treatment means grows. Formally, we can use an arbitrary significance threshold and compare our observed F-statistic against the associated F critical value to declare significance or not. Or, better than this, we could compute the p-value the corresponds to the observed F-statistic and then use that as a measure of evidence against the null hypothesis of equal means (the smaller the p-value, the more likely it becomes that the null hypothesis is not true).  

## Before you do a formal analysis ...

Let's look at an example one-way ANOVA. Before we actually perform a formal one-way ANOVA test, we need to do a few things, so let's do those things now. 

### Get the data into R

One of my roles at the institution where I work is as a consultant to a research centre that focuses on phytopathology and plant epidemiology in the context of agricultural crops. A portion of the researchers I work with concentrate on studying the impacts of microscopic worms called nematodes on different aspects of plant health. With that in mind, I'm going to construct (borrow) an artificial example in the guise of a nematode study. 

Suppose that a phytopathologist is interested in studying the impacts of different population densities of a certain nematode species on the growth of tomato seedlings. In consultation with their biometrician they decide on a completely randomised design in which they will introduce 4 levels of population density of the nematodes into 16 pots to be planted with the same type of tomato seed, so that in the end each level of nematode population density is assigned at random to 4 pots. 

This data could be recorded in a variety of different files, usually something like an Excel spreadsheet or a comma-separated, tab-delimited or other kind of delimited file. To get access to this data in R it needs to be imported or input by hand. It should typically be input in "tidy" or "long-form" with each column corresponding to a single variable, and each row corresponding to a single case or unit of observation (not necessarily the same as an experimental unit). 

There are a variety of ways to access the data from within R. If the data is saved as a `*.txt` file we use:
```{r, eval=FALSE}
my_data <- read.delim( file.choose() )
```
This provides an interactive means of accessing the particular file where the data is stored. Alternatively, you can pre-determine the path to your file and use:
```{r, eval=FALSE}
path_to_my_file <- "C://path/to/my/file/data.txt"
my_data <- read.delim( file=path_to_my_file )
```
where, obviously, the specific path to your file should be used. 

If the data is saved as a `*.csv` file we could use:
If the data is saved as a `*.txt` file we use:
```{r, eval=FALSE}
# Interactive option:
my_data <- read.csv( file.choose() )

# Or path specification option:
path_to_my_file <- "C://path/to/my/file/data.csv"
my_data <- read.csv( file=path_to_my_file )
```

If the data is saved as an excel spreadsheet file, we need to install/load the `readxl` package and then use the `read_excel()` function as follows:
```{r, eval=FALSE}
# Get pacman to manage install/loads of packages
if(! require(pacman)) install.packages("pacman")

# Load the readxl package
pacman::p_load(readxl)

# Import the data:
path_to_my_file <- "C://path/to/my/file/data.xlsx"
my_data <- read_excel( 
  path=path_to_my_file,
  sheet=NULL, 
  range=NULL,
  col_names=TRUE,
  na="NA")
```

In this particular instance, however, I'm going to input the data by hand. To do this, I'm going to install/load the suite of `tidyverse` packages, and the `knitr` and `kableExtra` packages:
```{r, include=FALSE}
if(!require(pacman)){ install.packages("pacman")}
```
```{r, results=FALSE}
pacman::p_load(tidyverse, knitr, kableExtra)
```
Then I'm going to use a tidy `tibble` to construct a tidy data frame:
```{r}
my_data <- tibble(
  nematode_density = c(0, 1000, 5000, 10000) %>% 
    rep(each=4) %>%
    ordered(),
  seedling_growth = c(10.8, 9.1, 13.5, 9.2, 
                      11.1, 11.1, 8.2, 11.3,
                       5.4, 4.6, 7.4, 5.0,
                       5.8, 5.3, 3.2, 7.5 )
)
```
This produces the data shown in the scroll box below.  
```{r, echo=FALSE}
my_data %>% 
  kable(
    caption = "Raw data for seedling growth under different nematode densities",
    col.names = c("Nematodes", "Growth (cm)"),
    align=c("l", "c"),
    padding = 40) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover"),
    font_size = 10, 
    position = "center")
```

### Do some basic pre-analysis checks

Once data is imported or input into R, we need to subject it to scrutiny. We will check to see that R has read the data in and assigned correct types, and will look for missing values and so on. To glimpse the data use:
```{r}
dplyr::glimpse(my_data)
```

We see that there are $N=16$ observations with each observation taken on an experimental experimental unit. There are two variables, a categorical ordinal factor `nematode_density` and  continuous measurement `seedling_growth`. The response variable $y_{ij}$ here is the increase in height of the tomato seedlings (measured in centimetres) 16 days after planting. We can determine the number of treatment groups using `levels()` and `nlevels()` as follows:
```{r}
with(my_data, levels(nematode_density))
with(my_data, nlevels(nematode_density))
```
This tells us that there are $K=4$ treatment groups corresponding to zero nematodes, 1000 nematodes, 5000 nematodes and 10000 nematodes. To determine whether the data is balanced, use `group_by()` and `summarise()` from the `dplyr` package:
```{r}
my_data_grouped <- my_data %>% 
  group_by(nematode_density)

summary_grouped <- my_data_grouped %>% 
  summarise(count = n(),
            mean = mean(seedling_growth),
            sd = sd(seedling_growth)
            )
```
```{r, echo=FALSE}
summary_grouped %>% 
  kable(
    caption = "Summary of group means and standard deviations.",
    col.names = c("Nematodes", "Reps", "Mean", "SD"),
    align = c("l", "c", "c", "c"),
    digits = 2,
    padding = 1000) %>%
  kable_styling(font_size=10)
```
Since the numbers in the __Replicates__ column are all equal, we know this data is balanced with $r=4$ replicates of each treatment. Also, we can look at the empirical group standard deviations in the __SD__ column to observe that the smallest such standard deviation is `r summary_grouped$sd %>% min() %>% round(2)` while the largest one is `r summary_grouped$sd %>% max() %>% round(2)`. Since the ratio of the largest to the smallest standard deviation is not greater than 2, as a rule of thumb, we can be informally confident that the group populations have homogenous variances. Notice, too, that there appears to be a decreasing trend in the means as we move from having uncontaminated soil, to having up to 10000 nematodes in the soil.This is probably what we expect, since the usual contention is that the presence of nematodes has a negative impact on plant growth.      

### Visualise the data

A picture paints a thousand words, and we can easily explore the data visually using the `ggplot2` package. We first install the latest CRAN release of `ggplot2` as follows:
```{r}
pacman::p_load(ggplot2)
```
Then we'll use side-by-side boxplots to get a feel for the data.
```{r}
# The basic boxplot
bxp_1 <- my_data %>%
  ggplot(mapping = aes(x = nematode_density,
                       y = seedling_growth,
                       fill = nematode_density), 
         col = "black") +
  geom_boxplot(
    outlier.colour = "red"
    )

# Specify title, labels, caption etc.
bxp_2 <- bxp_1 +
  labs(
    title = "Distribution of seedling growth \n by level of nematode density",
    x = "Nematode Treatment", 
    y = "Growth of seedlings (cm)",
    fill = "Treatment Group",
    caption = "The negative impact of nematodes on seedling growth appears to occur \n as we move from having 1000 nematodes to having 5000 or more.") +
  theme(
    plot.title = element_text(color="red", size=13, face="bold.italic"),
    axis.title.x = element_text(color="blue", size=12, face="bold"),
    axis.title.y = element_text(color="#993333", size=12, face="bold")) +
  scale_fill_brewer(palette = "Dark2")
  
bxp_2
```

Now, let's produce a means plot:
```{r}
meanplot <- ggplot(data = my_data,
                   mapping = aes(x = nematode_density,
                                 y = seedling_growth))

meanplot_1 <- meanplot +
  stat_summary(geom = "point", fun = "mean", colour = "#6600CC", size = 3)

meanplot_2 <- meanplot_1 +
  geom_jitter(mapping = aes(colour = nematode_density), 
              width = 0.1,
              size = 2)

summary_grouped <- summary_grouped %>%
  mutate(me = qt(0.95, df=3)*sd/sqrt(count),
         lo = mean - me,
         hi = mean + me,
         x_start = nematode_density,
         x_end = c(nematode_density[-1], NA),
         y_start = mean,
         y_end = c(mean[-1], NA))

meanplot_3 <- meanplot_2 +
  geom_errorbar(data = summary_grouped,
                mapping = aes(y = mean, ymin = lo, ymax = hi),
                colour = "#6600CC",
                width = 0.1,
                size = 1.1,
                alpha = 0.6)

meanplot_4 <- meanplot_3 +
  geom_segment(data = summary_grouped,
               mapping = aes(x=x_start, y=y_start, xend=x_end, yend = y_end),
               colour = "#6600CC",
               size = 1.1,
               alpha = 0.6)


meanplot_5 <- meanplot_4 +
  labs(x = "Nematode Treatment",
       y = "Seedling Growth (cm)",
       title = "Means plot of seedling growth by nematode density \n with LSD error bars",
       fill = "Treatment Group") +
  theme(plot.title = element_text(color="red", size=13, face="bold.italic"),
        axis.title.x = element_text(color="blue", size=12, face="bold"),
        axis.title.y = element_text(color="#993333", size=12, face="bold")) +
  scale_colour_brewer(palette = "Dark2")

meanplot_5
```

From these graphs it is fairly clear that we expect there to be a significant difference between means in an ANOVA test. Let's see, shall we?

## Perform the one-way ANOVA test

We want to know if there is any significant difference between the average seedling growth measure under each of the nematode treatments. The R function `aov()` can be used to carry out an ANOVA, while the function `summary.aov()` can be used to summarize the analysis of variance model. Morevoer, upon loading the `broom` package, the function `tidy()` can be used to coerce the output to a tibble for eas of access. 

```{r}
# Carry out a one-way ANOVA
m1_aov <- aov(seedling_growth ~ nematode_density, data = my_data)

# Summary of the analysis
summary(m1_aov)
```

To produce useful storage formats for the information from the ANOVA, do the following:
```{r}
pacman::p_load(broom, janitor)

m1_aov_modelstats <- glance(m1_aov) %>% clean_names()
m1_aov_modelstats

m1_aov_coefstats <- tidy(m1_aov) %>% clean_names()
m1_aov_coefstats

m1_aov_valuestats <- augment(m1_aov) %>% clean_names()
m1_aov_valuestats
```

## Do some basic post-analysis checks of ANOVA assumptions

The ANOVA test assumes that the data are normally distributed and the variance across groups is homogenous. Let's check that these assumptions are valid. 

### Check the assumption of homogeneity of variance

A plot of the residuals versus fitted values can be used to check the homogeneity of variances assumption. 
```{r}
# 1. Homogeneity of variances
plot(m1_aov, 1)
```

There is no major relationship between the residuals and the fitted values, and this indicates that the homogeneity of variance assumption is a relatively reasonable assumption to make. 

For further evidence, we can use the `leveneTest()` function from the `car` package to carry out a formal test for variance equality. 
```{r}
pacman::p_load(car)
leveneTest(seedling_growth ~ nematode_density, data = my_data)
```
Since the p-value is quite large (0.8072) there is little evidence against the null hypothesis of equal variances across treatment groups. Hence, we are safe to assume homogeneity of variances across treatment groups. 

### Check the normality assumption

The plot below is a normal Q-Q plot, which shows the empirical quantiles of the residuals plotted against the theoretical quantiles of the normal distribution. A 45-degree reference line is also plotted, demonstrating what a perfect fit to a normal distribution would look like. 
```{r}
plot(m1_aov, 2)
```

This shows us that the model (standardized) residuals are reasonably well modelled by an assumption of normality, since all the points fall approximately along the reference line. 

A Shapiro-Wilks test can be performed using `shapiro.test()` to formally test for normality. 

```{r}
# Extract the residuals
m1_aov_resids <- residuals(object = m1_aov)

# Shapiro-Wilk test
shapiro.test(x = m1_aov_resids)
```

The Shapiro-Wilk test leads to the same conclusion as the normal quantile plot, namely that the data are normally distributed ($W = 0.97, \, p = 0.86$).

## Interpret the result of the one-way ANOVA tests

The post-analysis checks of the anova model assumptions confirm that the assumptions are met reasonably well by the data. This gives us the green light to interpret the anova output. The p-value ($\approx 0.0006$) of the ANOVA F-test is very small, indicating that the observing data as extreme as this under the null hypothesis is quite rare. This leads us to reject the null hypothesis and conclude that there is a difference in mean seedling growth between the different nematode density treatments.  

## Planned contrasts and post-hoc multiple comparisons

### Specifying contrasts for ANOVA
A one-way ANOVA F-test is an omnibus test: it tells us that there is a difference (or not) between the treatment means. We have to go further to determine which of the specific means differs from the others. Often we have planned comparisons to make. Because there are four levels of the factor, we can make a total of 3 orthogonal contrasts. So, for example, suppose that the researcher wishes to compare the mean of all the experimental treatments against the mean of the control. That is:
\[
\begin{aligned}
H_0 &: \mu_1 - \frac{1}{3}(\mu_2 +\mu_3 +\mu_4) = 0 \\
H_1 &: \mu_1 - \frac{1}{3}(\mu_2 +\mu_3 +\mu_4) \neq 0.
\end{aligned}
\]
This corresponds to a vector of contrast coefficients as shown:
```{r}
# Contrast 0 vs mean(1000, 5000, 10000)
c1 <- c(3, -1, -1, -1)
```
Suppose too that the researcher is interested in comparing the experimental treatments as lowest versus the mean of the two highest, and highest versus next-highest. That is:
\[
\begin{aligned}
H_0 &: \mu_2 - \mu_3 = 0 \\
H_1 &: \mu_2 - \mu_3 \neq 0.
\end{aligned}
\]
and
\[
\begin{aligned}
H_0 &: \mu_3 - \mu_4) = 0 \\
H_1 &: \mu_3 - \mu_4) \neq 0.
\end{aligned}
\]
These correspond to contrast coefficient vectors:
```{r}
# Contrast 1000 vs mean(5000, 10000) 
c2 <- c(0, 1, -1, 0)
# Contrast 5000 vs 10000
c3 <- c(0, 0, 1, -1)
```
In R we need to set these individual contrasts up into a matrix of contrast coefficients and assign that matrix as the contrasts for the treatment variale of interest. We use `cbind()` to construct the contrast matrix with each of the planned contrasts being a column. We then use `contrasts()` to assign the contrasts to the treatment factor of interest in the data.
```{r}
# Build the contrast matrix
contrast_matrix <- cbind(c1, c2, c3)
# Assign the contrast matrix to nematode_density treatment factor
contrasts(my_data$nematode_density) <- contrast_matrix
```
Now we need to fit a new ANOVA model using the `aov()` function. Because we have already defined the contrasts to use, we don't need to do anything special to fit the model. R will just fit the model using those contrasts. 
```{r}
# Fit an anova model
m2_aov <- aov(seedling_growth ~ nematode_density, data = my_data)
```
The special thing we do need to do is let R know how to output the results in the `summary.aov()` function. Because we have three contrasts, we need to tell `summary.aov()` to split the summary accordingly. We do this be creating a list of a list as follows:
```{r}
# How to split summary of results
split_list <- list(
  nematode_density = list(
    "Control vs All Experimental" = 1,
    "1000 vs 5000)" = 2,
    "5000 vs 10000" = 3))
# Run summary.aov with that split list
summary.aov(m2_aov, split = split_list)
```
The output shows that `nematode_density` is highly significant on 3 degrees of freedom (p = 0.00616), and then proceeds to split the treatment sum of squares up into the three single degree of freedom contrasts we defined. This tells us that there is a significant experimental effect since the average experimental treatment mean differs significantly from the control (p = 0.003457). It also informs us that there is a significant difference between the 1000 and 5000 nematode treatments (p = 0.001487) but the difference between the 5000 and the 10000 nematode density is far less significant (p = 0.02736). This indicates that the tomato seedlings can cope with some nematodes in the soil, but as the density increases from 1000 up to 5000 nematodes, something pathological takes place. The jump from 5000 to 1000 doesn't appear to be as problematic though, indicating a kind of saturation threshold after which the seedling growth response is less sensitive to an increase in the nematode density. This is confirmed from our exploratory graphs created earlier.

### Tukey multiple pairwise-comparisons

We've only been able to test three orthogonal contrasts as planned comparisons. However, our study leads us to want to look at further tests. We can compute Tukey Honest Significant Differences (Tukey HSD) using the `TukeyHSD()` function, which takes a fitted ANOVA model as its argument.This computes all pairwise comparisons and provides Tukey-adjusted p-values to account for the fact that we are making multiple comparisons. We set the family-wise confidence level using the argument `conf.level`. 
```{r}
hsd <- TukeyHSD(x = m1_aov,
                ordered = TRUE,
                conf.level = 0.95)
hsd
```
We can also use `plot()` to induce the base plot method for Tukey HSD comparisons, to get a visual comparison of means.
```{r}
par(cex.axis = 0.5, mar = c(5, 5, 4, 2) + 0.1)
plot(hsd, las = 2)
```
This plot needs refinement before it can be used in a publication, but it is useful to get a quick feel for what the data indicates. Here we see that there is no significant difference between 0 and 1000 or 5000 and 10000. But the 0 vs. 5000, 0 vs. 10000, 1000 vs. 5000 and 1000 vs. 10000 comparisons are significantly different, as we have already seen from the orthogonal contrasts fit earlier.  

### Multiple comparisons using multcomp package
There is a package called `multcomp` that is dedicated to multiple comparisons in linear models (as well as more general models). We use the `glht()` function, which stands for "general linear hypothesis tests". The basic format is:
```{r, eval=FALSE}
glht(model, lincft)
```
where `model` is a fitted model object (e.g. returned by `aov()`) and `lincft` specifies the linear hypotheses to be tested. Multiple comparisons in ANOVA models are specified by objects returned by the function `mcp()`. For example, to perform all pairwise comparisons using the Tukey method we would use the following code.
```{r}
pacman::p_load(multcomp)

glht_hsd <- glht(model = m1_aov,
     linfct = mcp(nematode_density = "Tukey"))
summary(glht_hsd)
```
Simultaneous confidence intervals corresponding to comparisons are available via the `confint()` function:
```{r}
ci95 <- confint(glht_hsd)
ci95
```
In addition, we can produce a compact letter display to show groups of signficant comparisons using `cld()`. This function can take the outputs of either `summary.glht()`, `glht()` or `confint.glht()` and obviously, for the latter, there is no need to define the confidence level since it is inherited from the confidence interval procedure. Let's look at these now:
```{r}
cld(object = glht_hsd,
    level = 0.05,
    decreasing = TRUE)
```
This readily shows what we have learned, namely that the 0 and 1000 treatments seem to sit in a single group, and the 5000 and 10000 treatments seem to sit their own group. It might be interesting to specifically make the contrast of the average of 0 and 1000 agains the average of 5000 and 10000, and then look at 0 vs 1000 and 5000 vs 10000. We can conduct contrast comparisons using `multcomp` too, as follows:
```{r}
# Set up contrasts
c1 <- c(1, -1, 0, 0)
c2 <- c(0.5, 0.5, -0.5, -0.5)
c3 <- c(0, 0, 1, -1)
# Construct contrast matrix
## NB. rbind not cbind for contrast matrices in multcomp:
## A multcomp contrast matrix is the transpose of what 
## would be given to contrasts() in the base aov() 
## function. 
new_contrast_matrix <- rbind(c1, c2, c3) 
row.names(new_contrast_matrix) = c(
  "0 - 1000",
  "(0,1000) - (5000,10000)",
  "5000 - 10000")

glht_contrasts <- glht(model = m1_aov,
     linfct = mcp(nematode_density = new_contrast_matrix))
summary(glht_contrasts)
```
We can see that this explicitly shows the significant difference occurring between the 0 and 1000 group and the 5000 and 10000 group, as indicated by the lettering.  

### Pairwise t-tests
There is also functionality in R to carry out pairwise t-tests with corrections for multiple testing, using the `pairwise.t.test()` function.
```{r}
with(my_data,
     pairwise.t.test(
       x = seedling_growth, 
       g = nematode_density,
       p.adjust.method = "BH",
       pool.sd = TRUE))
```
The result is a table of p-values for the pairwise comparisons, adjusted using the Benjamini-Hochberg method, which controls the false discovery rate --- a less stringent condition than the familywise error rate, leading to a more powerful test. To control familywise error rate instead we could have used `p.adjust.method = "holm"` as a reasonable choice (although there are other methods too, see `? p.adjust` for details). 

## What to do if the ANOVA assumptions are not valid

When we look at the assumptions for ANOVA we may find that they are not valid. 

### Heterogenous variances

* __ANOVA test without homogeneity of variance assumption__

An alternative ANOVA procedure for the one-way layout is the Welch one-way test accessible via `oneway.test()`. 
```{r}
oneway.test(seedling_growth ~ nematode_density, 
            data = my_data)
```

Also, pairwise t-tests can be set to not use pooled standard deviation (`pool.sd = FALSE`) as a way of relaxing the assumption of variance homogeneity. 

### ... and Distributions not normal

When the normality assumtion is not met it is a matter of judgement as to whether we can use ANOVA. The sensitivity to a lack of normality is less than it is to heterogenous variances or a lack of independence. However, we can employ a nonparametric approach for the one-way layout called the Kruskal-Wallis rank-sum test when the ANOVA assumptions are not met. In R we use the function `kruskal.test()` as follows:
```{r}
kruskal.test(seedling_growth ~ nematode_density,
             data = my_data)
```
This shows a significant difference between the treatment medians. 

## Summary

1. Import your data
2. Visualise your data with a view to assessing the ANOVA assumptions as far as you can.
3. Perform one-way ANOVA test and check assumptions using residuals, residual plots and other tests.
4. If the ANOVA assumptions are met and the ANOVA fit is signficant, carry out planned and post-hoc multiple comparisons (the most consistent approach is using the `multcomp` package). If the ANOVA assumptions are not met use Welch one-way test, or non-parameteric alternative, the Kruskal-Wallis rank sum test. 
5. Interpret the results.








