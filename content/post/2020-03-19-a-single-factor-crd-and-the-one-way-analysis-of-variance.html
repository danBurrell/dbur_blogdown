---
title: A single factor CRD and the one-way analysis of variance
author: Daniel Burrell
date: '2020-04-03'
slug: a-single-factor-crd-and-the-one-way-analysis-of-variance
categories:
  - R
  - analysis of variance
  - design of experiments
  - completely randomised design
tags:
  - CRD
  - ANOVA
  - DOE
description: ''
topics: []
---

<script src="/rmarkdown-libs/kePrint/kePrint.js"></script>


<p>I’m currently teaching a class on the statistical design of experiments, and we’re using SPSS as the statistical software to implement basic analyses of data from designed experiments at the moment. Shortly we’d like to switch to using R as the main statistical computing package. I also have a client who has asked me to teach her how to use R to perform basic one-way and two-way analysis of variance for data arising from completely randomised designs with single factor and 2-factorial treatment structures, respectively. This is child’s play, especially in R, but I thought I’d take the opportunity to work through a simple example for future use.</p>
<p>To give credit where it is due, I have borrowed from Montgomery (1991) <em>Design and Analysis of Experiments</em>. Oehlert (2010) <a href="http://users.stat.umn.edu/~gary/book/fcdae.pdf"><em>A First Course in Design and Analysis of Experiments</em></a>, Meier (2018) <a href="https://stat.ethz.ch/~meier/teaching/anova/"><em>ANOVA: A Short Intro Using R</em></a> and an online tutorial introduction to one-way ANOVA by <a href="http://www.sthda.com/english/wiki/one-way-anova-test-in-r">STHDA</a> to develop this post.</p>
<div id="what-is-the-one-way-anova-test" class="section level2">
<h2>What is the one-way ANOVA test</h2>
<p>The <strong>one-way analysis of variance (ANOVA)</strong> extends the two independent samples t-test to the case where there are more than two independent samples or groups to be compared. In a <strong>one-way ANOVA</strong> the data is organised into groups based on a single grouping variable, which is called, in the language of experimental design, a <strong>factor</strong>. For this reason the one-way ANOVA is sometimes referred to as a <em>one-factor</em> or <em>single-factor ANOVA</em>. The groups usually correspond to different treatment conditions. The single factor is often referred to as the <strong>treatment</strong> or <strong>treatment factor</strong> and the different treatment conditions are referred to as its <strong>levels</strong>.</p>
<p>Let’s introduce some notation. Suppose we have a treatment factor with <span class="math inline">\(T\)</span> levels corresponding to <span class="math inline">\(T\)</span> distinct treatment conditions applied to <span class="math inline">\(N\)</span> experimental units in a completely randomised fashion such that each of the treatment conditions gets assigned to <span class="math inline">\(r_i\)</span> experimental units, where
<span class="math display">\[
\sum_{i=1}^T r_i = N.
\]</span>
The value <span class="math inline">\(r_j\)</span> is the sample size of the <span class="math inline">\(j\)</span>th treatment group, and is called it number of replicates. The optimal choice (with respect to statistical power) of the <span class="math inline">\(r_j\)</span> for all <span class="math inline">\(j\)</span> actually depends on the research question, but for the case where <span class="math inline">\(r_1 = r_2 = \cdots = r_T = r\)</span> (i.e. each treatment group has an equal number of replicates, or all the treatment groups have the same sample size), the design is said to be <strong>balanced</strong>.</p>
<p>The goal of the ANOVA is to compare the impacts of <span class="math inline">\(T&gt;2\)</span> treatments on some response measurement. We do this by formulating a parametric model for our data. The classical model is called the <strong>cell means model</strong>. Let <span class="math inline">\(Y_{ij}\)</span> denote the <span class="math inline">\(j\)</span>th replicate observation in treatment group <span class="math inline">\(i\)</span>, where <span class="math inline">\(i = 1,\ldots,T\)</span> and <span class="math inline">\(j=1,\ldots, r_i\)</span>. In the <strong>cell means model</strong> we allow each treatment group to have its own <em>expected value</em> but we assume that the observations are independent and fluctuate around this value according to a normal distribution, i.e.,
<span class="math display">\[
Y_{ij} \sim N(\mu_i, \sigma^2),\quad \text{independent}
\]</span>
where</p>
<ul>
<li><span class="math inline">\(\mu_i = \mathbb E[Y_{ij}]\)</span>, the expected value of the response random variable for treatment group <span class="math inline">\(i\)</span></li>
<li><span class="math inline">\(\sigma^2 = \mathbb V[Y_{ij}]\)</span>, the variance of response random variable, which is assumed to be homogenous or constant across all treatment groups.</li>
</ul>
<p>The above can be re-written as
<span class="math display">\[
Y_{ij} = \mu_i + \epsilon_{ij},
\]</span>
with (random) <em>errors</em> <span class="math inline">\(\epsilon_{ij} \sim N(0, \sigma^2)\)</span> by simply partitioning the <span class="math inline">\(N(\mu_i, \sigma^2)\)</span> distribution into a deterministic part <span class="math inline">\(\mu_i\)</span> and a stochastic part <span class="math inline">\(\epsilon_{ij}\)</span> fluctuating around <em>zero</em>. This clearly links to a simple linear regression, with <span class="math inline">\(Y\)</span> being the <strong>response</strong> and the treatment factor encoding the grouping information being a categorical <strong>predictor</strong>. The one-way ANOVA is nothing more than a regression model with a categorical predictor and normally distributed errors.</p>
<p>The categorical predictor or factor can be either unordered (<strong>nominal</strong>) or ordered (<strong>ordinal</strong>). For example, south-east Queensland wheat variety would be an unordered (nominal) factor (e.g. with levels “Strzlecki”, “EGA Wylie” and “Baxter”). There is no sense of ordering between these different varieties. Glyphosate concentration could be an ordered (ordinal) factor (e.g. with ordered levels “1 percent solution”, “2 percent solution”, “5 percent solution” and “10 percent solution”). There is clearly an order relation on the levels of Glyphosate concentration; we expect <span class="math inline">\(1\% &lt; 2\% &lt; 5\% &lt; 10\%\)</span> in terms of the impact of the glyphosate as a herbicide.</p>
<p>It’s also possible to further re-write the deterministic part as
<span class="math display">\[
\mu_i = \mu + \tau_i, \quad (i=1,\ldots,T)
\]</span>
to obtain a model structure of the form
<span class="math display">\[
Y_{ij} = \mu+\tau_i+\epsilon_{ij},
\]</span>
again with <span class="math inline">\(\epsilon_{ij} \sim N(0, \sigma^2)\)</span></p>
<p>This is the <strong>effects model</strong> where <span class="math inline">\(\tau_i\)</span> is the <span class="math inline">\(i\)</span>th <strong>treatment effect</strong>. Think of <span class="math inline">\(\mu\)</span> as a “global mean” and <span class="math inline">\(\tau_i\)</span> as a “deviation from the global mean due to the effect of the <span class="math inline">\(i\)</span>th treatment”. Actually, this interpretation is not always correct, but it is helpful.</p>
<p>Let’s look more closely at the parameters of these two models (cell means and effects models). The cell means model requires us to estimate <span class="math inline">\(\mu_1, \ldots,\mu_T\)</span> and <span class="math inline">\(\sigma^2\)</span> for a total of <span class="math inline">\(T+1\)</span> parameters. But the effects model requires us to estimate <span class="math inline">\(\mu\)</span>, <span class="math inline">\(\tau_1,\ldots,\alpha_T\)</span> and <span class="math inline">\(\sigma^2\)</span> for a total of <span class="math inline">\(T+2\)</span> parameters. The addition of the extra parameter in the effects model renders it <strong>non-identifiable</strong> (because we have <span class="math inline">\(T+2\)</span> parameters and only <span class="math inline">\(T+1\)</span> independent bits of information with which to model them. In other words, we are free to “shift around” effects between <span class="math inline">\(\mu\)</span> and the <span class="math inline">\(\alpha_i\)</span>’s without altering the resulting values of <span class="math inline">\(\mu_i\)</span>. For example, we are free to add a constant to <span class="math inline">\(\mu\)</span>, say <span class="math inline">\(\mu + c\)</span> and then adjust the <span class="math inline">\(\tau_i\)</span>’s by subtracting the same constant, <span class="math inline">\(\tau_i - c\)</span>, and this will lead to the same value of <span class="math inline">\(\mu_i\)</span> for each <span class="math inline">\(i\)</span>. Because of this non-identifiability problem, we need to impose a constraint on the <span class="math inline">\(\tau_i\)</span>’s that effectively “removes” the additional parameter and restores identifiability. Some of the commonly adopted constraints are:</p>
<ul>
<li><strong>sum-to-zero constraint:</strong> <span class="math inline">\(\quad \sum_{i=1}^T \tau_i = 0\)</span> leading to the interpretation that <span class="math inline">\(\quad \mu = \frac{1}{T} \sum_{i=1}^T \mu_i\)</span> (NB. In R this corresponds to the <code>contr.sum</code> method).</li>
<li><strong>weighted sum-to-zero constraint:</strong> <span class="math inline">\(\quad \sum_{i=1}^T r_i\tau_i = 0\)</span> leading to the interpretation that <span class="math inline">\(\mu = \frac{1}{N} \sum_{i=1}^T r_i\mu_i\)</span>.</li>
<li><strong>reference group constraint:</strong> <span class="math inline">\(\tau_1 = 0\)</span> leading to the interpretation that <span class="math inline">\(\mu = \mu_1\)</span> (NB. In R this corresponds to the default <code>contr.treatment</code> method).</li>
</ul>
<p>Only <span class="math inline">\(T-1\)</span> elements of the treatment effects are allowed to freely vary. If we know <span class="math inline">\(T-1\)</span> of the <span class="math inline">\(\tau_i\)</span> values, then we automatically know the remaining <span class="math inline">\(\tau_i\)</span> value. This fact is encoded in the treatment <strong>degrees of freedom (df)</strong>: we say that the treatment effects has <span class="math inline">\(T-1\)</span> degrees of freedom.</p>
<p>Without going into full details, we may estimate the parameters of the model using the <strong>least squares criterion</strong> which minimizes the squared deviation from the observed data <span class="math inline">\(y_{ij}\)</span> to the model values <span class="math inline">\(\mu+\tau_i\)</span>, i.e., for the cell means model:
<span class="math display">\[
\hat\mu_i = \arg\min_{\mu_i} \sum_{i=1}^T\sum_{j=1}^{r_i} (y_{ij} - \mu_i)^2,
\]</span>
and for the effects model:
<span class="math display">\[
\hat\mu,\hat\tau_i = \arg\min_{\mu,\tau_i} \sum_{i=1}^T\sum_{j=1}^{r_i} (y_{ij} - \mu - \tau_i)^2.
\]</span></p>
<p>We use the following notation:</p>
<ul>
<li>sum of group <span class="math inline">\(i\)</span>: <span class="math inline">\(\quad y_{i\cdot}=\sum_{j=1}^{r_i} y_{ij}\)</span></li>
<li>sum of all observations: <span class="math inline">\(\quad y_{\cdot\cdot}=\sum_{i=1}^T\sum_{j=1}^{r_i} y_{ij}\)</span></li>
<li>mean of group <span class="math inline">\(i\)</span>: <span class="math inline">\(\quad \bar y_{i\cdot}=\frac{1}{r_i}\sum_{j=1}^{r_i} y_{ij}\)</span></li>
<li>overall (total/grand/global) mean: <span class="math inline">\(\quad \bar y_{\cdot\cdot}=\frac{1}{N}\sum_{i=1}^T\sum_{j=1}^{r_i} y_{ij}\)</span></li>
</ul>
<p>As we can independently estimate the values of the different group means, we get that <span class="math inline">\(\hat \mu_i = \bar y_{i\cdot}\)</span>, so that:
<span class="math display">\[
\hat \mu_i = \hat \mu + \hat \tau_i = \bar y_i.
\]</span>
Depending on the side-constraint that we use we get different results for <span class="math inline">\(\hat\tau_i\)</span>.</p>
<p>We use the <strong>mean square error</strong> <span class="math inline">\(MS_E\)</span> to estimate the error variance <span class="math inline">\(\sigma^2\)</span>, giving:
<span class="math display">\[
\hat\sigma^2 = MS_E = \frac{1}{N-T}SS_E,
\]</span>
where <span class="math inline">\(SS_E\)</span> is the <strong>residual</strong> or <strong>error sum of squares</strong>:
<span class="math display">\[
SS_E = \sum_{i=1}^T\sum_{j=1}^{r_i} (y_{ij} - \hat \mu_i)^2.
\]</span>
A little algebra illustrates the relationship between these values and the sample variance of the observations in each treatment group. We can write the mean square error as:
<span class="math display">\[
MS_E = \frac{1}{N-T} \sum_{i=1}^T(r_i - 1)s_i^2,
\]</span>
since <span class="math inline">\(s_i^2\)</span> is by definition:
<span class="math display">\[
s_i^2 = \frac{1}{r_i - 1} \sum_{j=1}^{r_i} (y_{ij} - \hat\mu_i)^2.
\]</span>
The denominator <span class="math inline">\(N-T\)</span> in <span class="math inline">\(\hat\sigma^2 = MS_E\)</span> ensures that as an estimator, it unbiasedly estimates the error variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>Before we look at an example, let’s first review in a less mathematical and more verbally descriptive form, the core details of the ANOVA.</p>
</div>
<div id="assumptions-of-anova-test" class="section level2">
<h2>Assumptions of ANOVA test</h2>
<p>A one-way ANOVA test can only be applied when the following assumptions are met:</p>
<ul>
<li>The observations are obtained independently and randomly from the population defined by the factor levels. That is, each factor level identifies an independent, random sample from the population of interest.</li>
<li>The data of each factor level are normally distributed. That is, the populations from which the independent samples are randomly drawn under each treatment group are normally distributed.</li>
<li>These normal populations have a common variance. That is, despite the distributions of different treatment groups being allowed to have different means or expected values, the normal variance is meant to be constant across all treatment groups.</li>
</ul>
<p>The third assumption is theoretically sensible when you think that in an experiment local control has been exercised and the assignment of treatments to experimental units has been at random. We are selecting from a larger population of <em>presumably</em> relatively homogeous experimental units, and then the only thing that should be different about them, apart from inherent random variability (captured by the error variance), is whatever the applied treatments do to “shift” the treatment means away from the underlying overall mean. So, it’s reasonable to expect that the variance of each treatment group is identical to the inherent random variability of the underlying population, namely the random error variance, since we only expect treatments to impact the normal mean and not its variance. Of course, the mechanism by which the treatment impacts the response may be less docile than our assumptions allow, and so in practice this assumption (and the others) may not hold. In terms of robustness, it turns out that the most important assumption is independence, followed by homogeneity of variance. The way in which we design and conduct the experiment is intended to force independence as much as possible. We can use <strong>Levene’s test</strong> and side-by-side boxplots to make judgements about the variability. Also, a general rule of thumb suggests that if the largest standard deviation of a treatment group is not more than twice as large as the smallest standard deviation of a treatment group, then we ought not be overly concerned about variance heterogeniety (Of course I must confess I haven’t any real idea what the basis of this rule of thumb is, so I am hesitant to propagate it). To assess normality we can use kernel density plots and normal quantile-quantile plots id there is sufficient data. There are a variety of formal tests too. Common ones include the Kolmogorov-Smirnov test and the Shapiro-Wilks test. More will be said about testing the validity of the ANOVA assumptions in a later section.</p>
</div>
<div id="how-one-way-anova-test-works" class="section level2">
<h2>How one-way ANOVA test works</h2>
<p>If you’re anything like me, you might still be wondering why on earth we use an analysis of <em>variance</em> to compare <em>means</em>!
Well, firstly, the hypotheses we seek to test using a one-way ANOVA F-test are as follows:</p>
<p><span class="math display">\[
\begin{aligned}
H_0 &amp;: \mu_1 = \mu_2 = \cdots = \mu_T \\
H_1 &amp;: \mu_k \neq \mu_l \quad \text{for at least one pair of treatment means}
\end{aligned}
\]</span></p>
<p>The ability to do this rests on the assumptions of homogeneity of variance in normal populations. Conditional on the null hypothesis being true (i.e. if it is true), we can come up with two distinct variance estimators that should both estimate the same population parameter, namely the error variance <span class="math inline">\(\sigma^2\)</span>. We’ve already seen the first of these, the <strong>mean square error <span class="math inline">\(MS_E\)</span></strong>, which is the (weighted) average of the sample variances (i.e. the pooled sample variance). This measures the <strong>variance within samples</strong> or the <strong>variation that is inherent and due to random error</strong>. But we can also derive an estimator of the <strong>variance between samples</strong>, a weighted measure of the variance of the sample means. This measures the <strong>variation due to treatments or explained variation</strong>. But under the null hypothesis, and crucially, if the assumption of homogeneity of variance is met, the distributions should be exactly the same, so these two estimators should have similar values (they’re estimating the same underlying construct). We build these two estimators as follows. The <span class="math inline">\(MS_E\)</span> is built by calculating the <span class="math inline">\(SS_E\)</span> and dividing by the degrees of freedom for error, as shown earlier. In a similar fashion, we can compute a <strong>treatment sum of squares <span class="math inline">\((SS_T)\)</span></strong> and divide it by the degrees of freedom for treatments to get our variance estimator, the <strong>mean square for treatments <span class="math inline">\((MS_T)\)</span></strong>. Informally, under the null hypothesis, we expect that these two independent estimators of the error variance should have a ratio of 1. That is, <strong>if the null hypothesis is true</strong>, we expect:
<span class="math display">\[
\frac{MS_T}{MS_E}\approx 1
\]</span>
But because we’re assuming normality we can make this argument formal. Both these variance estimators can be algebraically re-arranged to enable us, via <strong>Cochran’s theorem</strong>, to claim that they are random variables having independent (central) chi-square <span class="math inline">\(\chi^2\)</span> distributions with corresponding degrees of freedom (i.e. for <span class="math inline">\(MS_T\)</span> we have <span class="math inline">\(\nu_1 = T-1\)</span> and for <span class="math inline">\(MS_E\)</span> we have <span class="math inline">\(\nu_2 = N-T\)</span> degrees of freedom). This is good because the ratio of two independent (central) chi-square distributed random variables has an F distribution, so that the ratio of the variation due to treatments (between samples) to the variation due to error (within samples) should be F-distributed with <span class="math inline">\(\nu_1\)</span> numerator degrees of freedom and <span class="math inline">\(\nu_2\)</span> denominator degrees of freedom.</p>
<p>In summary, to compare two or more treatment means using one-way ANOVA:</p>
<ol style="list-style-type: decimal">
<li>Compute the common variance, which is called the variance within samples, error variance or residual variance as <span class="math inline">\(\hat\sigma^2_{E} = MS_E\)</span> with <span class="math inline">\(\nu_1 = N-T\)</span> degrees of freedom.</li>
<li>Compute the variance between sample means as <span class="math inline">\(\hat\sigma^2_{T}\)</span> with <span class="math inline">\(\nu_2 = T-1\)</span> degrees of freedom.</li>
<li>Produce the observed F-statistic as the ratio <span class="math inline">\(\frac{\hat\sigma^2_{T}}{\hat\sigma^2_{E}}\sim F_{\nu_1,\nu_2}\)</span>.</li>
</ol>
<p>Then, informally, an observed F-statistic less than or equal to 1 indicates no significant difference between the treament means, but as the F-statistic increases beyond 1 the strength of evidence for a difference between treatment means grows. Formally, we can use an arbitrary significance threshold and compare our observed F-statistic against the associated F critical value to declare significance or not. Or, better than this, we could compute the p-value the corresponds to the observed F-statistic and then use that as a measure of evidence against the null hypothesis of equal means (the smaller the p-value, the more likely it becomes that the null hypothesis is not true).</p>
</div>
<div id="before-you-do-a-formal-analysis" class="section level2">
<h2>Before you do a formal analysis …</h2>
<p>Let’s look at an example one-way ANOVA. Before we actually perform a formal one-way ANOVA test, we need to do a few things, so let’s do those things now.</p>
<div id="get-the-data-into-r" class="section level3">
<h3>Get the data into R</h3>
<p>One of my roles at the institution where I work is as a consultant to a research centre that focuses on phytopathology and plant epidemiology in the context of agricultural crops. A portion of the researchers I work with concentrate on studying the impacts of microscopic worms called nematodes on different aspects of plant health. With that in mind, I’m going to construct (borrow) an artificial example in the guise of a nematode study.</p>
<p>Suppose that a phytopathologist is interested in studying the impacts of different population densities of a certain nematode species on the growth of tomato seedlings. In consultation with their biometrician they decide on a completely randomised design in which they will introduce 4 levels of population density of the nematodes into 16 pots to be planted with the same type of tomato seed, so that in the end each level of nematode population density is assigned at random to 4 pots.</p>
<p>This data could be recorded in a variety of different files, usually something like an Excel spreadsheet or a comma-separated, tab-delimited or other kind of delimited file. To get access to this data in R it needs to be imported or input by hand. It should typically be input in “tidy” or “long-form” with each column corresponding to a single variable, and each row corresponding to a single case or unit of observation (not necessarily the same as an experimental unit).</p>
<p>There are a variety of ways to access the data from within R. If the data is saved as a <code>*.txt</code> file we use:</p>
<pre class="r"><code>my_data &lt;- read.delim( file.choose() )</code></pre>
<p>This provides an interactive means of accessing the particular file where the data is stored. Alternatively, you can pre-determine the path to your file and use:</p>
<pre class="r"><code>path_to_my_file &lt;- &quot;C://path/to/my/file/data.txt&quot;
my_data &lt;- read.delim( file=path_to_my_file )</code></pre>
<p>where, obviously, the specific path to your file should be used.</p>
<p>If the data is saved as a <code>*.csv</code> file we could use:
If the data is saved as a <code>*.txt</code> file we use:</p>
<pre class="r"><code># Interactive option:
my_data &lt;- read.csv( file.choose() )

# Or path specification option:
path_to_my_file &lt;- &quot;C://path/to/my/file/data.csv&quot;
my_data &lt;- read.csv( file=path_to_my_file )</code></pre>
<p>If the data is saved as an excel spreadsheet file, we need to install/load the <code>readxl</code> package and then use the <code>read_excel()</code> function as follows:</p>
<pre class="r"><code># Get pacman to manage install/loads of packages
if(! require(pacman)) install.packages(&quot;pacman&quot;)

# Load the readxl package
pacman::p_load(readxl)

# Import the data:
path_to_my_file &lt;- &quot;C://path/to/my/file/data.xlsx&quot;
my_data &lt;- read_excel( 
  path=path_to_my_file,
  sheet=NULL, 
  range=NULL,
  col_names=TRUE,
  na=&quot;NA&quot;)</code></pre>
<p>In this particular instance, however, I’m going to input the data by hand. To do this, I’m going to install/load the suite of <code>tidyverse</code> packages, and the <code>knitr</code> and <code>kableExtra</code> packages:</p>
<pre class="r"><code>pacman::p_load(tidyverse, knitr, kableExtra)</code></pre>
<p>Then I’m going to use a tidy <code>tibble</code> to construct a tidy data frame:</p>
<pre class="r"><code>my_data &lt;- tibble(
  nematode_density = c(0, 1000, 5000, 10000) %&gt;% 
    rep(each=4) %&gt;%
    ordered(),
  seedling_growth = c(10.8, 9.1, 13.5, 9.2, 
                      11.1, 11.1, 8.2, 11.3,
                       5.4, 4.6, 7.4, 5.0,
                       5.8, 5.3, 3.2, 7.5 )
)</code></pre>
This produces the data shown in the scroll box below.<br />

<table class="table table-striped table-hover" style="font-size: 10px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:unnamed-chunk-8">Table 1: </span>Raw data for seedling growth under different nematode densities
</caption>
<thead>
<tr>
<th style="text-align:left;">
Nematodes
</th>
<th style="text-align:center;">
Growth (cm)
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
0
</td>
<td style="text-align:center;">
10.8
</td>
</tr>
<tr>
<td style="text-align:left;">
0
</td>
<td style="text-align:center;">
9.1
</td>
</tr>
<tr>
<td style="text-align:left;">
0
</td>
<td style="text-align:center;">
13.5
</td>
</tr>
<tr>
<td style="text-align:left;">
0
</td>
<td style="text-align:center;">
9.2
</td>
</tr>
<tr>
<td style="text-align:left;">
1000
</td>
<td style="text-align:center;">
11.1
</td>
</tr>
<tr>
<td style="text-align:left;">
1000
</td>
<td style="text-align:center;">
11.1
</td>
</tr>
<tr>
<td style="text-align:left;">
1000
</td>
<td style="text-align:center;">
8.2
</td>
</tr>
<tr>
<td style="text-align:left;">
1000
</td>
<td style="text-align:center;">
11.3
</td>
</tr>
<tr>
<td style="text-align:left;">
5000
</td>
<td style="text-align:center;">
5.4
</td>
</tr>
<tr>
<td style="text-align:left;">
5000
</td>
<td style="text-align:center;">
4.6
</td>
</tr>
<tr>
<td style="text-align:left;">
5000
</td>
<td style="text-align:center;">
7.4
</td>
</tr>
<tr>
<td style="text-align:left;">
5000
</td>
<td style="text-align:center;">
5.0
</td>
</tr>
<tr>
<td style="text-align:left;">
10000
</td>
<td style="text-align:center;">
5.8
</td>
</tr>
<tr>
<td style="text-align:left;">
10000
</td>
<td style="text-align:center;">
5.3
</td>
</tr>
<tr>
<td style="text-align:left;">
10000
</td>
<td style="text-align:center;">
3.2
</td>
</tr>
<tr>
<td style="text-align:left;">
10000
</td>
<td style="text-align:center;">
7.5
</td>
</tr>
</tbody>
</table>
</div>
<div id="do-some-basic-pre-analysis-checks" class="section level3">
<h3>Do some basic pre-analysis checks</h3>
<p>Once data is imported or input into R, we need to subject it to scrutiny. We will check to see that R has read the data in and assigned correct types, and will look for missing values and so on. To glimpse the data use:</p>
<pre class="r"><code>dplyr::glimpse(my_data)</code></pre>
<pre><code>## Observations: 16
## Variables: 2
## $ nematode_density &lt;ord&gt; 0, 0, 0, 0, 1000, 1000, 1000, 1000, 5000, 5000, 50...
## $ seedling_growth  &lt;dbl&gt; 10.8, 9.1, 13.5, 9.2, 11.1, 11.1, 8.2, 11.3, 5.4, ...</code></pre>
<p>We see that there are <span class="math inline">\(N=16\)</span> observations with each observation taken on an experimental experimental unit. There are two variables, a categorical ordinal factor <code>nematode_density</code> and continuous measurement <code>seedling_growth</code>. The response variable <span class="math inline">\(y_{ij}\)</span> here is the increase in height of the tomato seedlings (measured in centimetres) 16 days after planting. We can determine the number of treatment groups using <code>levels()</code> and <code>nlevels()</code> as follows:</p>
<pre class="r"><code>with(my_data, levels(nematode_density))</code></pre>
<pre><code>## [1] &quot;0&quot;     &quot;1000&quot;  &quot;5000&quot;  &quot;10000&quot;</code></pre>
<pre class="r"><code>with(my_data, nlevels(nematode_density))</code></pre>
<pre><code>## [1] 4</code></pre>
<p>This tells us that there are <span class="math inline">\(K=4\)</span> treatment groups corresponding to zero nematodes, 1000 nematodes, 5000 nematodes and 10000 nematodes. To determine whether the data is balanced, use <code>group_by()</code> and <code>summarise()</code> from the <code>dplyr</code> package:</p>
<pre class="r"><code>my_data_grouped &lt;- my_data %&gt;% 
  group_by(nematode_density)

summary_grouped &lt;- my_data_grouped %&gt;% 
  summarise(count = n(),
            mean = mean(seedling_growth),
            sd = sd(seedling_growth)
            )</code></pre>
<table class="table" style="font-size: 10px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:unnamed-chunk-12">Table 2: </span>Summary of group means and standard deviations.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Nematodes
</th>
<th style="text-align:center;">
Reps
</th>
<th style="text-align:center;">
Mean
</th>
<th style="text-align:center;">
SD
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
0
</td>
<td style="text-align:center;">
4
</td>
<td style="text-align:center;">
10.65
</td>
<td style="text-align:center;">
2.05
</td>
</tr>
<tr>
<td style="text-align:left;">
1000
</td>
<td style="text-align:center;">
4
</td>
<td style="text-align:center;">
10.43
</td>
<td style="text-align:center;">
1.49
</td>
</tr>
<tr>
<td style="text-align:left;">
5000
</td>
<td style="text-align:center;">
4
</td>
<td style="text-align:center;">
5.60
</td>
<td style="text-align:center;">
1.24
</td>
</tr>
<tr>
<td style="text-align:left;">
10000
</td>
<td style="text-align:center;">
4
</td>
<td style="text-align:center;">
5.45
</td>
<td style="text-align:center;">
1.77
</td>
</tr>
</tbody>
</table>
<p>Since the numbers in the <strong>Replicates</strong> column are all equal, we know this data is balanced with <span class="math inline">\(r=4\)</span> replicates of each treatment. Also, we can look at the empirical group standard deviations in the <strong>SD</strong> column to observe that the smallest such standard deviation is 1.24 while the largest one is 2.05. Since the ratio of the largest to the smallest standard deviation is not greater than 2, as a rule of thumb, we can be informally confident that the group populations have homogenous variances. Notice, too, that there appears to be a decreasing trend in the means as we move from having uncontaminated soil, to having up to 10000 nematodes in the soil.This is probably what we expect, since the usual contention is that the presence of nematodes has a negative impact on plant growth.</p>
</div>
<div id="visualise-the-data" class="section level3">
<h3>Visualise the data</h3>
<p>A picture paints a thousand words, and we can easily explore the data visually using the <code>ggplot2</code> package. We first install the latest CRAN release of <code>ggplot2</code> as follows:</p>
<pre class="r"><code>pacman::p_load(ggplot2)</code></pre>
<p>Then we’ll use side-by-side boxplots to get a feel for the data.</p>
<pre class="r"><code># The basic boxplot
bxp_1 &lt;- my_data %&gt;%
  ggplot(mapping = aes(x = nematode_density,
                       y = seedling_growth,
                       fill = nematode_density), 
         col = &quot;black&quot;) +
  geom_boxplot(
    outlier.colour = &quot;red&quot;
    )

# Specify title, labels, caption etc.
bxp_2 &lt;- bxp_1 +
  labs(
    title = &quot;Distribution of seedling growth \n by level of nematode density&quot;,
    x = &quot;Nematode Treatment&quot;, 
    y = &quot;Growth of seedlings (cm)&quot;,
    fill = &quot;Treatment Group&quot;,
    caption = &quot;The negative impact of nematodes on seedling growth appears to occur \n as we move from having 1000 nematodes to having 5000 or more.&quot;) +
  theme(
    plot.title = element_text(color=&quot;red&quot;, size=13, face=&quot;bold.italic&quot;),
    axis.title.x = element_text(color=&quot;blue&quot;, size=12, face=&quot;bold&quot;),
    axis.title.y = element_text(color=&quot;#993333&quot;, size=12, face=&quot;bold&quot;)) +
  scale_fill_brewer(palette = &quot;Dark2&quot;)
  
bxp_2</code></pre>
<p><img src="/post/2020-03-19-a-single-factor-crd-and-the-one-way-analysis-of-variance_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>Now, let’s produce a means plot:</p>
<pre class="r"><code>meanplot &lt;- ggplot(data = my_data,
                   mapping = aes(x = nematode_density,
                                 y = seedling_growth))

meanplot_1 &lt;- meanplot +
  stat_summary(geom = &quot;point&quot;, fun = &quot;mean&quot;, colour = &quot;#6600CC&quot;, size = 3)

meanplot_2 &lt;- meanplot_1 +
  geom_jitter(mapping = aes(colour = nematode_density), 
              width = 0.1,
              size = 2)

summary_grouped &lt;- summary_grouped %&gt;%
  mutate(me = qt(0.95, df=3)*sd/sqrt(count),
         lo = mean - me,
         hi = mean + me,
         x_start = nematode_density,
         x_end = c(nematode_density[-1], NA),
         y_start = mean,
         y_end = c(mean[-1], NA))

meanplot_3 &lt;- meanplot_2 +
  geom_errorbar(data = summary_grouped,
                mapping = aes(y = mean, ymin = lo, ymax = hi),
                colour = &quot;#6600CC&quot;,
                width = 0.1,
                size = 1.1,
                alpha = 0.6)

meanplot_4 &lt;- meanplot_3 +
  geom_segment(data = summary_grouped,
               mapping = aes(x=x_start, y=y_start, xend=x_end, yend = y_end),
               colour = &quot;#6600CC&quot;,
               size = 1.1,
               alpha = 0.6)


meanplot_5 &lt;- meanplot_4 +
  labs(x = &quot;Nematode Treatment&quot;,
       y = &quot;Seedling Growth (cm)&quot;,
       title = &quot;Means plot of seedling growth by nematode density \n with LSD error bars&quot;,
       fill = &quot;Treatment Group&quot;) +
  theme(plot.title = element_text(color=&quot;red&quot;, size=13, face=&quot;bold.italic&quot;),
        axis.title.x = element_text(color=&quot;blue&quot;, size=12, face=&quot;bold&quot;),
        axis.title.y = element_text(color=&quot;#993333&quot;, size=12, face=&quot;bold&quot;)) +
  scale_colour_brewer(palette = &quot;Dark2&quot;)

meanplot_5</code></pre>
<pre><code>## Warning: Removed 1 rows containing missing values (geom_segment).</code></pre>
<p><img src="/post/2020-03-19-a-single-factor-crd-and-the-one-way-analysis-of-variance_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>From these graphs it is fairly clear that we expect there to be a significant difference between means in an ANOVA test. Let’s see, shall we?</p>
</div>
</div>
<div id="perform-the-one-way-anova-test" class="section level2">
<h2>Perform the one-way ANOVA test</h2>
<p>We want to know if there is any significant difference between the average seedling growth measure under each of the nematode treatments. The R function <code>aov()</code> can be used to carry out an ANOVA, while the function <code>summary.aov()</code> can be used to summarize the analysis of variance model. Morevoer, upon loading the <code>broom</code> package, the function <code>tidy()</code> can be used to coerce the output to a tibble for eas of access.</p>
<pre class="r"><code># Carry out a one-way ANOVA
m1_aov &lt;- aov(seedling_growth ~ nematode_density, data = my_data)

# Summary of the analysis
summary(m1_aov)</code></pre>
<pre><code>##                  Df Sum Sq Mean Sq F value   Pr(&gt;F)    
## nematode_density  3 100.65   33.55   12.08 0.000616 ***
## Residuals        12  33.33    2.78                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>To produce useful storage formats for the information from the ANOVA, do the following:</p>
<pre class="r"><code>pacman::p_load(broom, janitor)

m1_aov_modelstats &lt;- glance(m1_aov) %&gt;% clean_names()
m1_aov_modelstats</code></pre>
<pre><code>## # A tibble: 1 x 11
##   r_squared adj_r_squared sigma statistic p_value    df log_lik   aic   bic
##       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1     0.751         0.689  1.67      12.1 6.16e-4     4   -28.6  67.1  71.0
## # ... with 2 more variables: deviance &lt;dbl&gt;, df_residual &lt;int&gt;</code></pre>
<pre class="r"><code>m1_aov_coefstats &lt;- tidy(m1_aov) %&gt;% clean_names()
m1_aov_coefstats</code></pre>
<pre><code>## # A tibble: 2 x 6
##   term                df sumsq meansq statistic   p_value
##   &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
## 1 nematode_density     3 101.   33.5       12.1  0.000616
## 2 Residuals           12  33.3   2.78      NA   NA</code></pre>
<pre class="r"><code>m1_aov_valuestats &lt;- augment(m1_aov) %&gt;% clean_names()
m1_aov_valuestats</code></pre>
<pre><code>## # A tibble: 16 x 9
##    seedling_growth nematode_density fitted se_fit  resid   hat sigma  cooksd
##              &lt;dbl&gt; &lt;ord&gt;             &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;
##  1            10.8 0                 10.6   0.833  0.150 0.25   1.74 9.00e-4
##  2             9.1 0                 10.6   0.833 -1.55  0.250  1.65 9.61e-2
##  3            13.5 0                 10.6   0.833  2.85  0.25   1.43 3.25e-1
##  4             9.2 0                 10.6   0.833 -1.45  0.250  1.67 8.41e-2
##  5            11.1 1000              10.4   0.833  0.675 0.25   1.72 1.82e-2
##  6            11.1 1000              10.4   0.833  0.675 0.25   1.72 1.82e-2
##  7             8.2 1000              10.4   0.833 -2.23  0.25   1.56 1.98e-1
##  8            11.3 1000              10.4   0.833  0.875 0.25   1.71 3.06e-2
##  9             5.4 5000               5.6   0.833 -0.200 0.25   1.74 1.60e-3
## 10             4.6 5000               5.6   0.833 -1     0.25   1.71 4.00e-2
## 11             7.4 5000               5.6   0.833  1.8   0.25   1.62 1.30e-1
## 12             5   5000               5.6   0.833 -0.6   0.25   1.73 1.44e-2
## 13             5.8 10000              5.45  0.833  0.35  0.25   1.74 4.90e-3
## 14             5.3 10000              5.45  0.833 -0.15  0.25   1.74 9.00e-4
## 15             3.2 10000              5.45  0.833 -2.25  0.25   1.55 2.03e-1
## 16             7.5 10000              5.45  0.833  2.05  0.25   1.59 1.68e-1
## # ... with 1 more variable: std_resid &lt;dbl&gt;</code></pre>
</div>
<div id="do-some-basic-post-analysis-checks-of-anova-assumptions" class="section level2">
<h2>Do some basic post-analysis checks of ANOVA assumptions</h2>
<p>The ANOVA test assumes that the data are normally distributed and the variance across groups is homogenous. Let’s check that these assumptions are valid.</p>
<div id="check-the-assumption-of-homogeneity-of-variance" class="section level3">
<h3>Check the assumption of homogeneity of variance</h3>
<p>A plot of the residuals versus fitted values can be used to check the homogeneity of variances assumption.</p>
<pre class="r"><code># 1. Homogeneity of variances
plot(m1_aov, 1)</code></pre>
<p><img src="/post/2020-03-19-a-single-factor-crd-and-the-one-way-analysis-of-variance_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>There is no major relationship between the residuals and the fitted values, and this indicates that the homogeneity of variance assumption is a relatively reasonable assumption to make.</p>
<p>For further evidence, we can use the <code>leveneTest()</code> function from the <code>car</code> package to carry out a formal test for variance equality.</p>
<pre class="r"><code>pacman::p_load(car)
leveneTest(seedling_growth ~ nematode_density, data = my_data)</code></pre>
<pre><code>## Levene&#39;s Test for Homogeneity of Variance (center = median)
##       Df F value Pr(&gt;F)
## group  3  0.3252 0.8072
##       12</code></pre>
<p>Since the p-value is quite large (0.8072) there is little evidence against the null hypothesis of equal variances across treatment groups. Hence, we are safe to assume homogeneity of variances across treatment groups.</p>
</div>
<div id="check-the-normality-assumption" class="section level3">
<h3>Check the normality assumption</h3>
<p>The plot below is a normal Q-Q plot, which shows the empirical quantiles of the residuals plotted against the theoretical quantiles of the normal distribution. A 45-degree reference line is also plotted, demonstrating what a perfect fit to a normal distribution would look like.</p>
<pre class="r"><code>plot(m1_aov, 2)</code></pre>
<p><img src="/post/2020-03-19-a-single-factor-crd-and-the-one-way-analysis-of-variance_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>This shows us that the model (standardized) residuals are reasonably well modelled by an assumption of normality, since all the points fall approximately along the reference line.</p>
<p>A Shapiro-Wilks test can be performed using <code>shapiro.test()</code> to formally test for normality.</p>
<pre class="r"><code># Extract the residuals
m1_aov_resids &lt;- residuals(object = m1_aov)

# Shapiro-Wilk test
shapiro.test(x = m1_aov_resids)</code></pre>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  m1_aov_resids
## W = 0.9713, p-value = 0.8592</code></pre>
<p>The Shapiro-Wilk test leads to the same conclusion as the normal quantile plot, namely that the data are normally distributed (<span class="math inline">\(W = 0.97, \, p = 0.86\)</span>).</p>
</div>
</div>
<div id="interpret-the-result-of-the-one-way-anova-tests" class="section level2">
<h2>Interpret the result of the one-way ANOVA tests</h2>
<p>The post-analysis checks of the anova model assumptions confirm that the assumptions are met reasonably well by the data. This gives us the green light to interpret the anova output. The p-value (<span class="math inline">\(\approx 0.0006\)</span>) of the ANOVA F-test is very small, indicating that the observing data as extreme as this under the null hypothesis is quite rare. This leads us to reject the null hypothesis and conclude that there is a difference in mean seedling growth between the different nematode density treatments.</p>
</div>
<div id="planned-contrasts-and-post-hoc-multiple-comparisons" class="section level2">
<h2>Planned contrasts and post-hoc multiple comparisons</h2>
<div id="specifying-contrasts-for-anova" class="section level3">
<h3>Specifying contrasts for ANOVA</h3>
<p>A one-way ANOVA F-test is an omnibus test: it tells us that there is a difference (or not) between the treatment means. We have to go further to determine which of the specific means differs from the others. Often we have planned comparisons to make. Because there are four levels of the factor, we can make a total of 3 orthogonal contrasts. So, for example, suppose that the researcher wishes to compare the mean of all the experimental treatments against the mean of the control. That is:
<span class="math display">\[
\begin{aligned}
H_0 &amp;: \mu_1 - \frac{1}{3}(\mu_2 +\mu_3 +\mu_4) = 0 \\
H_1 &amp;: \mu_1 - \frac{1}{3}(\mu_2 +\mu_3 +\mu_4) \neq 0.
\end{aligned}
\]</span>
This corresponds to a vector of contrast coefficients as shown:</p>
<pre class="r"><code># Contrast 0 vs mean(1000, 5000, 10000)
c1 &lt;- c(3, -1, -1, -1)</code></pre>
<p>Suppose too that the researcher is interested in comparing the experimental treatments as lowest versus the mean of the two highest, and highest versus next-highest. That is:
<span class="math display">\[
\begin{aligned}
H_0 &amp;: \mu_2 - \mu_3 = 0 \\
H_1 &amp;: \mu_2 - \mu_3 \neq 0.
\end{aligned}
\]</span>
and
<span class="math display">\[
\begin{aligned}
H_0 &amp;: \mu_3 - \mu_4) = 0 \\
H_1 &amp;: \mu_3 - \mu_4) \neq 0.
\end{aligned}
\]</span>
These correspond to contrast coefficient vectors:</p>
<pre class="r"><code># Contrast 1000 vs mean(5000, 10000) 
c2 &lt;- c(0, 1, -1, 0)
# Contrast 5000 vs 10000
c3 &lt;- c(0, 0, 1, -1)</code></pre>
<p>In R we need to set these individual contrasts up into a matrix of contrast coefficients and assign that matrix as the contrasts for the treatment variale of interest. We use <code>cbind()</code> to construct the contrast matrix with each of the planned contrasts being a column. We then use <code>contrasts()</code> to assign the contrasts to the treatment factor of interest in the data.</p>
<pre class="r"><code># Build the contrast matrix
contrast_matrix &lt;- cbind(c1, c2, c3)
# Assign the contrast matrix to nematode_density treatment factor
contrasts(my_data$nematode_density) &lt;- contrast_matrix</code></pre>
<p>Now we need to fit a new ANOVA model using the <code>aov()</code> function. Because we have already defined the contrasts to use, we don’t need to do anything special to fit the model. R will just fit the model using those contrasts.</p>
<pre class="r"><code># Fit an anova model
m2_aov &lt;- aov(seedling_growth ~ nematode_density, data = my_data)</code></pre>
<p>The special thing we do need to do is let R know how to output the results in the <code>summary.aov()</code> function. Because we have three contrasts, we need to tell <code>summary.aov()</code> to split the summary accordingly. We do this be creating a list of a list as follows:</p>
<pre class="r"><code># How to split summary of results
split_list &lt;- list(
  nematode_density = list(
    &quot;Control vs All Experimental&quot; = 1,
    &quot;1000 vs 5000)&quot; = 2,
    &quot;5000 vs 10000&quot; = 3))
# Run summary.aov with that split list
summary.aov(m2_aov, split = split_list)</code></pre>
<pre><code>##                                                 Df Sum Sq Mean Sq F value
## nematode_density                                 3 100.65   33.55  12.080
##   nematode_density: Control vs All Experimental  1  36.58   36.58  13.169
##   nematode_density: 1000 vs 5000)                1  46.56   46.56  16.765
##   nematode_density: 5000 vs 10000                1  17.51   17.51   6.305
## Residuals                                       12  33.33    2.78        
##                                                   Pr(&gt;F)    
## nematode_density                                0.000616 ***
##   nematode_density: Control vs All Experimental 0.003457 ** 
##   nematode_density: 1000 vs 5000)               0.001487 ** 
##   nematode_density: 5000 vs 10000               0.027360 *  
## Residuals                                                   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The output shows that <code>nematode_density</code> is highly significant on 3 degrees of freedom (p = 0.00616), and then proceeds to split the treatment sum of squares up into the three single degree of freedom contrasts we defined. This tells us that there is a significant experimental effect since the average experimental treatment mean differs significantly from the control (p = 0.003457). It also informs us that there is a significant difference between the 1000 and 5000 nematode treatments (p = 0.001487) but the difference between the 5000 and the 10000 nematode density is far less significant (p = 0.02736). This indicates that the tomato seedlings can cope with some nematodes in the soil, but as the density increases from 1000 up to 5000 nematodes, something pathological takes place. The jump from 5000 to 1000 doesn’t appear to be as problematic though, indicating a kind of saturation threshold after which the seedling growth response is less sensitive to an increase in the nematode density. This is confirmed from our exploratory graphs created earlier.</p>
</div>
<div id="tukey-multiple-pairwise-comparisons" class="section level3">
<h3>Tukey multiple pairwise-comparisons</h3>
<p>We’ve only been able to test three orthogonal contrasts as planned comparisons. However, our study leads us to want to look at further tests. We can compute Tukey Honest Significant Differences (Tukey HSD) using the <code>TukeyHSD()</code> function, which takes a fitted ANOVA model as its argument.This computes all pairwise comparisons and provides Tukey-adjusted p-values to account for the fact that we are making multiple comparisons. We set the family-wise confidence level using the argument <code>conf.level</code>.</p>
<pre class="r"><code>hsd &lt;- TukeyHSD(x = m1_aov,
                ordered = TRUE,
                conf.level = 0.95)
hsd</code></pre>
<pre><code>##   Tukey multiple comparisons of means
##     95% family-wise confidence level
##     factor levels have been ordered
## 
## Fit: aov(formula = seedling_growth ~ nematode_density, data = my_data)
## 
## $nematode_density
##             diff       lwr      upr     p adj
## 5000-10000 0.150 -3.348577 3.648577 0.9992199
## 1000-10000 4.975  1.476423 8.473577 0.0056301
## 0-10000    5.200  1.701423 8.698577 0.0040599
## 1000-5000  4.825  1.326423 8.323577 0.0070131
## 0-5000     5.050  1.551423 8.548577 0.0050470
## 0-1000     0.225 -3.273577 3.723577 0.9973921</code></pre>
<p>We can also use <code>plot()</code> to induce the base plot method for Tukey HSD comparisons, to get a visual comparison of means.</p>
<pre class="r"><code>par(cex.axis = 0.5, mar = c(5, 5, 4, 2) + 0.1)
plot(hsd, las = 2)</code></pre>
<p><img src="/post/2020-03-19-a-single-factor-crd-and-the-one-way-analysis-of-variance_files/figure-html/unnamed-chunk-28-1.png" width="672" />
This plot needs refinement before it can be used in a publication, but it is useful to get a quick feel for what the data indicates. Here we see that there is no significant difference between 0 and 1000 or 5000 and 10000. But the 0 vs. 5000, 0 vs. 10000, 1000 vs. 5000 and 1000 vs. 10000 comparisons are significantly different, as we have already seen from the orthogonal contrasts fit earlier.</p>
</div>
<div id="multiple-comparisons-using-multcomp-package" class="section level3">
<h3>Multiple comparisons using multcomp package</h3>
<p>There is a package called <code>multcomp</code> that is dedicated to multiple comparisons in linear models (as well as more general models). We use the <code>glht()</code> function, which stands for “general linear hypothesis tests”. The basic format is:</p>
<pre class="r"><code>glht(model, lincft)</code></pre>
<p>where <code>model</code> is a fitted model object (e.g. returned by <code>aov()</code>) and <code>lincft</code> specifies the linear hypotheses to be tested. Multiple comparisons in ANOVA models are specified by objects returned by the function <code>mcp()</code>. For example, to perform all pairwise comparisons using the Tukey method we would use the following code.</p>
<pre class="r"><code>pacman::p_load(multcomp)

glht_hsd &lt;- glht(model = m1_aov,
     linfct = mcp(nematode_density = &quot;Tukey&quot;))
summary(glht_hsd)</code></pre>
<pre><code>## 
##   Simultaneous Tests for General Linear Hypotheses
## 
## Multiple Comparisons of Means: Tukey Contrasts
## 
## 
## Fit: aov(formula = seedling_growth ~ nematode_density, data = my_data)
## 
## Linear Hypotheses:
##                   Estimate Std. Error t value Pr(&gt;|t|)   
## 1000 - 0 == 0       -0.225      1.178  -0.191  0.99739   
## 5000 - 0 == 0       -5.050      1.178  -4.285  0.00491 **
## 10000 - 0 == 0      -5.200      1.178  -4.413  0.00385 **
## 5000 - 1000 == 0    -4.825      1.178  -4.095  0.00723 **
## 10000 - 1000 == 0   -4.975      1.178  -4.222  0.00571 **
## 10000 - 5000 == 0   -0.150      1.178  -0.127  0.99922   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## (Adjusted p values reported -- single-step method)</code></pre>
<p>Simultaneous confidence intervals corresponding to comparisons are available via the <code>confint()</code> function:</p>
<pre class="r"><code>ci95 &lt;- confint(glht_hsd)
ci95</code></pre>
<pre><code>## 
##   Simultaneous Confidence Intervals
## 
## Multiple Comparisons of Means: Tukey Contrasts
## 
## 
## Fit: aov(formula = seedling_growth ~ nematode_density, data = my_data)
## 
## Quantile = 2.9683
## 95% family-wise confidence level
##  
## 
## Linear Hypotheses:
##                   Estimate lwr     upr    
## 1000 - 0 == 0     -0.2250  -3.7228  3.2728
## 5000 - 0 == 0     -5.0500  -8.5478 -1.5522
## 10000 - 0 == 0    -5.2000  -8.6978 -1.7022
## 5000 - 1000 == 0  -4.8250  -8.3228 -1.3272
## 10000 - 1000 == 0 -4.9750  -8.4728 -1.4772
## 10000 - 5000 == 0 -0.1500  -3.6478  3.3478</code></pre>
<p>In addition, we can produce a compact letter display to show groups of signficant comparisons using <code>cld()</code>. This function can take the outputs of either <code>summary.glht()</code>, <code>glht()</code> or <code>confint.glht()</code> and obviously, for the latter, there is no need to define the confidence level since it is inherited from the confidence interval procedure. Let’s look at these now:</p>
<pre class="r"><code>cld(object = glht_hsd,
    level = 0.05,
    decreasing = TRUE)</code></pre>
<pre><code>##     0  1000  5000 10000 
##   &quot;a&quot;   &quot;a&quot;   &quot;b&quot;   &quot;b&quot;</code></pre>
<p>This readily shows what we have learned, namely that the 0 and 1000 treatments seem to sit in a single group, and the 5000 and 10000 treatments seem to sit their own group. It might be interesting to specifically make the contrast of the average of 0 and 1000 agains the average of 5000 and 10000, and then look at 0 vs 1000 and 5000 vs 10000. We can conduct contrast comparisons using <code>multcomp</code> too, as follows:</p>
<pre class="r"><code># Set up contrasts
c1 &lt;- c(1, -1, 0, 0)
c2 &lt;- c(0.5, 0.5, -0.5, -0.5)
c3 &lt;- c(0, 0, 1, -1)
# Construct contrast matrix
## NB. rbind not cbind for contrast matrices in multcomp:
## A multcomp contrast matrix is the transpose of what 
## would be given to contrasts() in the base aov() 
## function. 
new_contrast_matrix &lt;- rbind(c1, c2, c3) 
row.names(new_contrast_matrix) = c(
  &quot;0 - 1000&quot;,
  &quot;(0,1000) - (5000,10000)&quot;,
  &quot;5000 - 10000&quot;)

glht_contrasts &lt;- glht(model = m1_aov,
     linfct = mcp(nematode_density = new_contrast_matrix))
summary(glht_contrasts)</code></pre>
<pre><code>## 
##   Simultaneous Tests for General Linear Hypotheses
## 
## Multiple Comparisons of Means: User-defined Contrasts
## 
## 
## Fit: aov(formula = seedling_growth ~ nematode_density, data = my_data)
## 
## Linear Hypotheses:
##                              Estimate Std. Error t value Pr(&gt;|t|)    
## 0 - 1000 == 0                  0.2250     1.1784   0.191    0.996    
## (0,1000) - (5000,10000) == 0   5.0125     0.8333   6.016   &lt;0.001 ***
## 5000 - 10000 == 0              0.1500     1.1784   0.127    0.999    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## (Adjusted p values reported -- single-step method)</code></pre>
<p>We can see that this explicitly shows the significant difference occurring between the 0 and 1000 group and the 5000 and 10000 group, as indicated by the lettering.</p>
</div>
<div id="pairwise-t-tests" class="section level3">
<h3>Pairwise t-tests</h3>
<p>There is also functionality in R to carry out pairwise t-tests with corrections for multiple testing, using the <code>pairwise.t.test()</code> function.</p>
<pre class="r"><code>with(my_data,
     pairwise.t.test(
       x = seedling_growth, 
       g = nematode_density,
       p.adjust.method = &quot;BH&quot;,
       pool.sd = TRUE))</code></pre>
<pre><code>## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  seedling_growth and nematode_density 
## 
##       0      1000   5000  
## 1000  0.9008 -      -     
## 5000  0.0022 0.0022 -     
## 10000 0.0022 0.0022 0.9008
## 
## P value adjustment method: BH</code></pre>
<p>The result is a table of p-values for the pairwise comparisons, adjusted using the Benjamini-Hochberg method, which controls the false discovery rate — a less stringent condition than the familywise error rate, leading to a more powerful test. To control familywise error rate instead we could have used <code>p.adjust.method = "holm"</code> as a reasonable choice (although there are other methods too, see <code>? p.adjust</code> for details).</p>
</div>
</div>
<div id="what-to-do-if-the-anova-assumptions-are-not-valid" class="section level2">
<h2>What to do if the ANOVA assumptions are not valid</h2>
<p>When we look at the assumptions for ANOVA we may find that they are not valid.</p>
<div id="heterogenous-variances" class="section level3">
<h3>Heterogenous variances</h3>
<ul>
<li><strong>ANOVA test without homogeneity of variance assumption</strong></li>
</ul>
<p>An alternative ANOVA procedure for the one-way layout is the Welch one-way test accessible via <code>oneway.test()</code>.</p>
<pre class="r"><code>oneway.test(seedling_growth ~ nematode_density, 
            data = my_data)</code></pre>
<pre><code>## 
##  One-way analysis of means (not assuming equal variances)
## 
## data:  seedling_growth and nematode_density
## F = 10.947, num df = 3.0000, denom df = 6.5645, p-value = 0.005904</code></pre>
<p>Also, pairwise t-tests can be set to not use pooled standard deviation (<code>pool.sd = FALSE</code>) as a way of relaxing the assumption of variance homogeneity.</p>
</div>
<div id="and-distributions-not-normal" class="section level3">
<h3>… and Distributions not normal</h3>
<p>When the normality assumtion is not met it is a matter of judgement as to whether we can use ANOVA. The sensitivity to a lack of normality is less than it is to heterogenous variances or a lack of independence. However, we can employ a nonparametric approach for the one-way layout called the Kruskal-Wallis rank-sum test when the ANOVA assumptions are not met. In R we use the function <code>kruskal.test()</code> as follows:</p>
<pre class="r"><code>kruskal.test(seedling_growth ~ nematode_density,
             data = my_data)</code></pre>
<pre><code>## 
##  Kruskal-Wallis rank sum test
## 
## data:  seedling_growth by nematode_density
## Kruskal-Wallis chi-squared = 11.355, df = 3, p-value = 0.009954</code></pre>
<p>This shows a significant difference between the treatment medians.</p>
</div>
</div>
<div id="summary" class="section level2">
<h2>Summary</h2>
<ol style="list-style-type: decimal">
<li>Import your data</li>
<li>Visualise your data with a view to assessing the ANOVA assumptions as far as you can.</li>
<li>Perform one-way ANOVA test and check assumptions using residuals, residual plots and other tests.</li>
<li>If the ANOVA assumptions are met and the ANOVA fit is signficant, carry out planned and post-hoc multiple comparisons (the most consistent approach is using the <code>multcomp</code> package). If the ANOVA assumptions are not met use Welch one-way test, or non-parameteric alternative, the Kruskal-Wallis rank sum test.</li>
<li>Interpret the results.</li>
</ol>
</div>
