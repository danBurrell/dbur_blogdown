---
title: Inverse Problems and Uncertainty Quantification
author: Daniel Burrell
date: '2020-03-19'
slug: inverse-problems-and-uncertainty-quantification
categories:
  - inverse problems
  - uncertainty quantification
  - UQ
  - VVUQ
  - Bayesian
tags:
  - UQ
  - VVUQ
  - Bayesian
  - Inverse Problems
description: ''
topics: []
---



<p>I’m interested in uncertainty quantification. There is a brief article in <a href="https://www.maths.nottingham.ac.uk/plp/pmzmi/siam.pdf">SIAM NEWS from 2014</a> which I just read that has piqued my interest further. The article, authored by Marco Iglesias and Andrew Stuart, discusses uncertainty quantification (UQ) in the context of inverse problems from a Bayesian perspective. Let’s have a look at the basic ideas set out in the article to get a feel for this important and “exciting area of research in the mathematical sciences” that brings together skills and knowledge from “analysis, computation, probability, and statistics.”</p>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>Inverse problems arise across diverse disciplines from the physical, biological and social sciences, to applications in climatology, epidemiology and fluid dynamics. I myself have a particularly diverse interest in inverse problems across vastly different fields, ranging from astronomy and astrophysics, to geophysical and atmospheric fluid dynamics, to biodynamics and mathematical biology, including epidemiology (of human, animal and plant diseases). Iglesias and Stuart (2014) describe inverse problems as follows:</p>
<blockquote>
<p>“… inverse problems confront mathematical models with data so that we can deduce the inputs needed to run the models; knowledge of these
inputs can then be used to make predictions, and even to devise control strategies based on the predictions. Both the models and the data
are typically uncertain, as are the resulting deductions and predictions; as a consequence, any decisions or control strategies based on
the predictions will be greatly improved if the uncertainty is made quantitative.”</p>
</blockquote>
<p>So, what they’re saying is that we often seek to develop mathematical models of real-life situations or phenomena, with the view to gaining some kind of predictive insight which we can then utilise to control said phenomenon. The problem is that models are not the real thing, they’re just (hopefully) useful fictions that abstract away unimportant details to focus in on the important ones in a given context. This abstraction introduces uncertainty in the model itself, so we hope to assess its fit to data and hence embark on a process of parameter estimation that is, in some sense, optimal. Of course, the data is imperfectly measured too, so another source of uncertainty creeps in, and it’s all well and good to agree with me that the models and the data are uncertain, but it would be far more useful if we could somehow quantify the uncertainty and the contributions from different sources, and how these uncertainties propagate through model runs and into the outputs. If we could do that, we could start to find ways to control the degree of uncertainty entering into the whole process and hopefuly improve our modelling and predictive capacity.</p>
</div>
<div id="bayesian-inverse-problems" class="section level2">
<h2>Bayesian inverse problems</h2>
<p>One of the schools of probability theory is the Bayesian school. Now while classical probability centres on probability as a long-run frequency, Bayesian are more inclined to view probability as a means of encoding uncertainties. A Bayesian perspective, then, is likely to be a useful one when it comes to quantifying uncertainties in inverse problems using probabilistic uncertainty models.</p>
<p>Consider a mathematical model <span class="math inline">\(M\)</span> of an experiment to be a set of theoretically or empirically informed equations that relate inputs <span class="math inline">\(u\)</span> (physical variables that can be manipulated prior to experimentation) to outputs <span class="math inline">\(y\)</span> (measurable quantities that in some sense vary in response to the manipulations of the inputs). Mathematicians distinguish between two broad kinds of problems in this context: <em>forward problems</em> and <em>inverse problems</em>. The forward problem uses model <span class="math inline">\(M\)</span> to predict the outputs <span class="math inline">\(y\)</span> of an experiment given its inputs <span class="math inline">\(u\)</span>, while the inverse problem uses the model <span class="math inline">\(M\)</span>, given measurements of the outputs <span class="math inline">\(y\)</span> (i.e. data), to make inferences about the model inputs <span class="math inline">\(u\)</span> (parameter estimation).</p>
<p>Mathematicians also distinguish between so-called <em>well-posed</em> and <em>ill-posed</em> problems and although there are today a number of different conceptions of the conditions that constitute ill-posedness and well-posedness (see, for example <a href="https://arxiv.org/pdf/1709.01109.pdf">Hofmann &amp; Plato (2018)</a>. Probably the original conception of a well-posed problem is due to the French mathematician Jacques Hadamard (1923), who considered that all mathematical problems pertaining to physical or technological realities ought to satisfy well-posedness conditions: existence, uniqueness and continuous dependence of the solution on the data. Problems are ill-posed (in the sense of Hadamard) if they fail to satisfy any one or more of these criteria (see the survey paper by <a href="http://math.nsc.ru/LBRT/u2/Survey%20paper.pdf">Kabanikhin (2008)</a> and the still informative paper by <a href="https://projecteuclid.org/euclid.ss/1177013525">O’Sullivan (1986)</a> for a more detailed discussion on the specifics of what these conditions entail). Without getting bogged down in the details, the take-home message is that inverse problems often are Hadamard ill-posed by virtue of noisy data and an imperfect model, and whenever we’re faced with such problems, it’s important to quantify the uncertainty inherent in any inferences and predictions that we make as part of our attempts at solving the inverse problem. Usually, to solve ill-posed problems some kind of additional information needs to be added to the scenario in the form of assumptions that lead to a well-defined solution space — a process called regularization. Adopting a Bayesian approach to inverse problems allows us to quantify uncertainty in a logical, consistent and principled way that is capable of simultaneously regularizing any ill-posedness because the Bayesian approach always requires the incorporation of additional background information encoded as prior probability distribution on the parameters (i.e. essentially a restriction to the solution space).</p>
<p>Let’s consider the Bayesian approach in more depth. Take the input-output pair <span class="math inline">\((u,y)\)</span> to be a random variable distributed according to a joint probability distribution <span class="math inline">\(L(u,y)\)</span>. Then a solution to the inverse problem would consist of the conditional distribution of the inputs <span class="math inline">\(u\)</span> given the outputs <span class="math inline">\(y\)</span>, say <span class="math inline">\(\pi(u|y)\)</span>, where we treat <span class="math inline">\(u|y\)</span> to be a conditional random variable. Then, by Bayes’ theorem, we can write:
<span class="math display">\[
\pi(u|y) \propto L(y|u)\pi_0(u).
\]</span>
Here we have had to add into the mix some additional background (or prior) information about the inputs <span class="math inline">\(u\)</span> through the <em>prior</em> distribution <span class="math inline">\(\pi_0(u)\)</span>. The probability distribution <span class="math inline">\(L(y|u)\)</span> is called the data <em>likelihood</em> and it encodes how likely the observed data <span class="math inline">\(y\)</span> is for given inputs <span class="math inline">\(u\)</span>. The probability distribution of <span class="math inline">\(u|y\)</span>, namely <span class="math inline">\(p(u|y)\)</span>, is called the <em>posterior</em> and encodes what we know about the unknown <span class="math inline">\(u\)</span> conditional on the (known) data <span class="math inline">\(y\)</span>. Bayes’ theorem explicitly says that the posterior distribution (i.e. our solution to the inverse problem) is equal to the likelihood multiplied by the prior, up to a normalizing constant. In the context of inverse problems, the prior encodes our present state of knowledge about the inputs in the input space and the model <span class="math inline">\(M\)</span> defines the likelihood. Then observed data <span class="math inline">\(y\)</span> is used to add new information into the mix so we can update our prior knowledge about the inputs, and this updated knowledge is encoded in the posterior. This updating of knowledge impacts the degree of uncertainty about a quantity of interest <span class="math inline">\(q\)</span>. Without data, we can estimate/predict our uncertainty about <span class="math inline">\(q\)</span> using the prior distribution. With data, we can make a more refined estimate/predicition of the uncertainty about <span class="math inline">\(q\)</span> from the relevant aspects of the posterior. The strong advantage of the Bayesian approach is the ease (in principle) with which we can encode our uncertainty about a quantity of interest and then bring data to bear on the situation in order to reduce the uncertainty in our predictions about <span class="math inline">\(q\)</span>. Everything in a Bayesian approach is uncertain, so uncertainty quantification via probability distributions is a natural artefact of this approach.</p>
</div>
<div id="a-note-of-caution" class="section level2">
<h2>A note of caution</h2>
<p>The statement of Bayes’ theorem is deceptively simple. In real-world inverse problems, there is a lot lurking within that simple looking statement. Iglesias and Stuart (2014) give the example the use of carbon capture and storage to facilitate global mitigation of the greenhouse effect, where interest is in assessing the economic viability and environmental impact of injecting carbon dioxide into Earth’s subsurface - for example, using a depleted oil or gas field as a carbon dioxide storage site. This example is based on a paper by Chadwick (2011) with the catchy title of <em>“Between a rock and a hard place?”</em> The mathematical model in this case consists of a suite of partial differential equations (PDEs) describing the <span class="math inline">\(\text{CO}_2\)</span> plume injected into the subsurface reservoir. Inputs to the model include parameters governing geological features such as the presence of faults and fractures and the permeability of the storage site, Outputs comprise the measurements of bottom-hole pressure from the injection well and other remotely-sensed satellite data such as measures of surface deformation. The key point is that the subsurface is not directly observable, so we have to infer its properties (inputs) with measurements (outputs) — an inverse problem.</p>
<p>The ability to make those inferences accurately and to quantify our uncertainty about them, will enable decisions to be made on the basis of a variety of quantities of interest surrounding the economic viability and environmental impact of such a strategy, but the practical complexity inherent to the use of Bayes’ theorem here is not immediately obvious when we just write down the theorem as stated earlier: <span class="math inline">\(\pi(u|y) \propto L(y|u)\pi_0(u)\)</span>. For a start, the likelihood is defined through solution of the forward model comprised of a suite of coupled PDEs (conservation laws) that describe multi-phase flow in a porous medium. If that’s not dautning enough, the probability distribution on the input space is defined over a space of functions (read as very high-dimensional probability space). To update our knowledge and access the posterior, we need to be able to solve complex partial differential equations over a dauntingly high-dimensional input space. This is typical of inverse problems in a wide range of applications.</p>
</div>
<div id="what-does-this-mean-practically" class="section level2">
<h2>What does this mean practically?</h2>
<ul>
<li>In terms of modelling: application specific; concerned with the choice of prior for the unknown</li>
<li>In terms of computation: concerns ways to get at the joint posterior distribution with reasonable accuracy to be able to compute the posterior probability distributions of the quantities of interest in reasonable computing time. The flexible MCMC methods are a natural methodology for sampling the posterior distribution, but they are practically hampered by their <span class="math inline">\(N^{-\frac{1}{2}}\)</span> convergence rate, which gives rise to excessive computational complexity (cost per unit error); multi-level Monte Carlo and quasi-Monte Carlow methods look promising; as do methods based on generalized polynomial chaos.</li>
</ul>
</div>
<div id="further-reading" class="section level2">
<h2>Further reading</h2>
<p>For more on the capabilities and limitations of MCMC and improved Monte Carlo methods, see:</p>
<ul>
<li><a href="https://www.math.ucla.edu/~caflisch/Pubs/Pubs1995-1999/actaNumerica1998.pdf">R. Caflisch, <em>Monte Carlo and quasi-Monte Carlo methods</em>, Acta Numer., 7 (1998), 1-49.</a></li>
<li><a href="https://arxiv.org/abs/1202.0709">S. Cotter, G. Roberts, A.M. Stuart, and D. White, <em>MCMC methods for functions: Modifying old algorithms to make them faster</em>, Stat. Sci., 28 (2013), 424-446.</a></li>
<li><a href="https://people.maths.ox.ac.uk/gilesm/files/OPRE_2008.pdf">M. Giles, <em>Multilevel Monte Carlo path simulation</em>, Oper. Res., 56 (2008), 607–617</a></li>
</ul>
<p>For more on generalized polynomial chaos and related methods, see:</p>
<ul>
<li><a href="https://www.researchgate.net/publication/220179440_Galerkin_Finite_Element_Approximations_of_Stochastic_Elliptic_Partial_Differential_Equations">I. Babuska, R. Tempone, and G. Zouraris, <em>Galerkin finite element approximations of stochastic elliptic partial differential
equations</em>, SIAM J. Numer. Anal., 42 (2004), 800–825</a></li>
<li><a href="https://arxiv.org/abs/1103.4522">C. Schwab and A.M. Stuart, <em>Sparse deterministic approximation of Bayesian inverse problems</em>, Inverse Problems, 28 (2012)</a>.</li>
</ul>
<p>For more on statistical approaches to inverse problems, see:</p>
<ul>
<li><a href="https://homepages.warwick.ac.uk/~masdr/BOOKCHAPTERS/stuart15c.pdf">A.M. Stuart, Inverse problems: A Bayesian perspective, Acta Numer., 19 (2010), 451–559.</a></li>
<li><a href="https://www.springer.com/gp/book/9780387220734">J. Kaipio and E. Somersalo, Statistical and Computational Inverse Problems, 160, Applied Mathematical Sciences, SpringerVerlag, New York, 2005</a></li>
</ul>
</div>
