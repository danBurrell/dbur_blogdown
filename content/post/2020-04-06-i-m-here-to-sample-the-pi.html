---
title: I'm here to sample the pi!
author: No Instance(s) Available.
date: '2020-04-06'
slug: i-m-here-to-sample-the-pi
categories:
  - Monte Carlo methods
tags:
  - Monte Carlo Methods
description: ''
topics: []
---



<p>In the very small amount of spare time that I have available, I’ve been trying to bring myself up to speed with Bayesian statistical models and in particular statistical sampling methods. As such I was watching <a href="https://www.youtube.com/watch?v=khagz6yWL9w&amp;list=PLqdbxUnkqOw2nKn7VxYqIrKWcqRkQYOsF&amp;index=7">this talk</a> by Iain Murray on probabilistic modelling and I saw him explain the basics of Monte Carlo integration using a simple toy example of estimating the value of <span class="math inline">\(\pi\)</span>. Mind you, he gave that particular slide the title “A dumb approximation of <span class="math inline">\(\pi\)</span>” because there are far better ways to do it. Nonetheless, I thought I’d implement the method myself, just for the sake of implementing a simple Monte Carlo method. So, without further ado…</p>
<div id="a-dumb-approximation-of-pi" class="section level2">
<h2>A dumb approximation of <span class="math inline">\(\pi\)</span></h2>
<p>Here’s the basic idea. Take a unit circle, so that <span class="math inline">\(r=1\)</span> and compute the area:
<span class="math display">\[
A = \pi\times r^2 = \pi\times 1^2 = \pi
\]</span></p>
<p>In his talk Iain actually used a quarter sector of a unit circle, but this only means that he integrated from the centre out to the boundary and multiplied the result by 4, but it will work to take a full circle and integrate over <span class="math inline">\(-1 &lt; x &lt; 1\)</span> and <span class="math inline">\(-1 &lt; y &lt; 1\)</span> too.</p>
<p>So we begin with a unit circle, but first let’s load the required packages.</p>
<pre class="r"><code>if(!require(pacman)){install.packages(&quot;pacman&quot;)}</code></pre>
<pre><code>## Loading required package: pacman</code></pre>
<pre class="r"><code>pacman::p_load(tidyverse, gridExtra, ggpubr)</code></pre>
<p><img src="/post/2020-04-06-i-m-here-to-sample-the-pi_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>But, we’re going to look at the situation slightly differently. Consider randomly tossing needles on the square defined by <span class="math inline">\(-1&lt;x&lt;1\)</span> and <span class="math inline">\(-1&lt;y&lt;1\)</span>, and counting the number of needles that fall within the unit circle. Think about the ratio of these two areas: the area of the circle <span class="math inline">\(A_c\)</span> and the are of the square <span class="math inline">\(A_s\)</span>. Then the ratio of the area of the circle with radius <span class="math inline">\(r\)</span> to the area of the square with side of length <span class="math inline">\(2r\)</span> we get:
<span class="math display">\[
\frac{A_c}{A_s} = \frac{\pi r^2}{(2r)^2}=\frac{\pi}{4}.
\]</span>
So, if we’re considering a unit circle <span class="math inline">\((r=1)\)</span> centered at the origin and the square with side lengths <span class="math inline">\(s=2r=2\)</span>, also centered at the origin, then we should be able keep track of the number of needles that fall inside the circle and the number that fall outside in the enveloping square. Then by multiplying the ratio of the internal to external needles by 4 we should be able to estimate the value of <span class="math inline">\(\pi\)</span>. Certainly, we expect that:
<span class="math display">\[
4\bigg[\lim_{n\rightarrow \infty} \frac{A^{(n)}_c}{A^{(n)}_s} \bigg] = \pi.
\]</span></p>
<p>We will set up a Monte Carlo sampling scheme, as follows: first set <span class="math inline">\(A_c^{(0)}= A_s^{(0)}=0\)</span> and then for <span class="math inline">\(n\in \mathbb Z, 1 \leq n \leq N\)</span> sample:
<span class="math display">\[
\begin{aligned}
x^{(n)} &amp;\sim U(-1,1) \\
y^{(n)}&amp; \sim U(-1,1)
\end{aligned}
\]</span>
if <span class="math inline">\(x^2 + y^2 \leq 1\)</span> then set <span class="math inline">\(A_c^{(n)}=A^{(n-1)}+1\)</span> and if <span class="math inline">\(x^2 + y^2 &gt; 1\)</span> then set <span class="math inline">\(A_s^{(n)}=A^{(n-1)}+1\)</span>. Compute the quantity
<span class="math display">\[
P^{(n)} = 4\times \frac{A_c^{(n)}}{A_s^{(n)}},
\]</span>
then plot the quantity <span class="math inline">\(P^{(n)}\)</span> against <span class="math inline">\(n\)</span> until the ratio converges (and we expect it to converge to <span class="math inline">\(\pi\)</span>). I coded this up into a function called <code>monte_pi()</code> which has the circle function <code>gg_circle()</code> embedded within. The <code>monte_pi()</code> functions takes the number of <code>samples</code> as an argument and returns side-by-side plots of the monte carlo simulation results. The left-hand plot shows the actual samples points coloured according to whether they fall inside the unit circle or outside it. The right-hand plot shows the value of the proportion of points falling inside the circle to the total number of points multiplied by a factor of 4, that is, the running estimate of <span class="math inline">\(P^{(n)} = 4\cdot \frac{A^{(n)}_c}{A^{(n)}_s}\approx \pi\)</span>.</p>
<pre class="r"><code>monte_pi &lt;- function(samples = 10, seed = 25, ...){
    
    if(!require(pacman)){install.packages(pacman)}
    pacman::p_load(tidyverse, gridExtra)
    set.seed(seed)
    monte_dat &lt;- tibble(
        x = runif(n = samples, min = -1, max = 1),
        y = runif(n = samples, min = -1, max = 1),
        d = sqrt(x^2 + y^2),
        n = 1:samples
) %&gt;%
    mutate(inside = ifelse(d &lt;= 1, 1, 0),
           num_in = cumsum(inside),
           prop = 4*(num_in/n))

# A function to compute a circle
gg_circle &lt;- function(r, xc, yc, color=&quot;black&quot;, fill=NA, ...) {
    x &lt;- xc + r*cos(seq(0, pi, length.out=100))
    ymax &lt;- yc + r*sin(seq(0, pi, length.out=100))
    ymin &lt;- yc + r*sin(seq(0, -pi, length.out=100))
    annotate(&quot;ribbon&quot;, x=x, ymin=ymin, ymax=ymax, color=color, fill=fill, ...)
}

#Construct the circle
circ_dat &lt;- tibble(x = 0:1,
              y = 0:1)

circle &lt;- ggplot(
    data = circ_dat, 
    mapping = aes(x=x, y=y)) +
    gg_circle(r = 1, 
              xc = 0, 
              yc = 0, 
              color=&quot;#F8F8FF&quot;, 
              fill=&quot;#F8F8FF&quot;) +
    geom_point(
        data = monte_dat,
        mapping = aes(x = x, 
                      y = y, 
                      colour = factor(inside), 
                      fill = factor(inside))) +
    theme(legend.position=&quot;none&quot;) +
    coord_equal(ratio = 1) +
    scale_colour_manual(values = c(&quot;red&quot;, &quot;blue&quot;))

dynamics &lt;- ggplot(
    data = monte_dat,
    mapping = aes(x = n, y = prop)) +
    geom_hline(yintercept = pi, linetype=&quot;dashed&quot;, color = &quot;magenta&quot;, size=1.2) +
    geom_line(color = &quot;blue&quot;) +
    ylab(expression(P^{(n)})) +
    theme(axis.text.y = element_text(angle = 0))

out_plot &lt;- arrangeGrob(circle, dynamics, ncol = 2)
monte_pi_out &lt;- list(df = monte_dat, p = out_plot)
return(monte_pi_out)
}</code></pre>
<p>Let’s see how well we’ve done. For <code>samples = 10</code> we get:</p>
<pre class="r"><code>mp10 &lt;- monte_pi(samples = 10)
as_ggplot(mp10$p)</code></pre>
<p><img src="/post/2020-04-06-i-m-here-to-sample-the-pi_files/figure-html/unnamed-chunk-4-1.png" width="672" />
with <span class="math inline">\(P^{(10)}=\)</span> 3.2. Increasing the number of samples to 30, we have:</p>
<pre class="r"><code>mp30 &lt;- monte_pi(samples = 30)
as_ggplot(mp30$p)</code></pre>
<p><img src="/post/2020-04-06-i-m-here-to-sample-the-pi_files/figure-html/unnamed-chunk-5-1.png" width="672" />
with <span class="math inline">\(P^{(30)}=\)</span> 3.2. Increasing to 100 samples, we get:</p>
<pre class="r"><code>mp100 &lt;- monte_pi(samples = 100)
as_ggplot(mp100$p)</code></pre>
<p><img src="/post/2020-04-06-i-m-here-to-sample-the-pi_files/figure-html/unnamed-chunk-6-1.png" width="672" />
with <span class="math inline">\(P^{(100)}=\)</span> 3.12. Increasing to 300 samples, we get:</p>
<pre class="r"><code>mp300 &lt;- monte_pi(samples = 300)
as_ggplot(mp300$p)</code></pre>
<p><img src="/post/2020-04-06-i-m-here-to-sample-the-pi_files/figure-html/unnamed-chunk-7-1.png" width="672" />
with <span class="math inline">\(P^{(300)}=\)</span> 3.04. Increasing to 500 samples, we get:</p>
<pre class="r"><code>mp500 &lt;- monte_pi(samples = 500)
as_ggplot(mp500$p)</code></pre>
<p><img src="/post/2020-04-06-i-m-here-to-sample-the-pi_files/figure-html/unnamed-chunk-8-1.png" width="672" />
with <span class="math inline">\(P^{(500)}=\)</span> 3.12. Increasing to 1500 samples, we get:</p>
<pre class="r"><code>mp1500 &lt;- monte_pi(samples = 1500)
as_ggplot(mp1500$p)</code></pre>
<p><img src="/post/2020-04-06-i-m-here-to-sample-the-pi_files/figure-html/unnamed-chunk-9-1.png" width="672" />
with <span class="math inline">\(P^{(1500)}=\)</span> 3.1386667. Increasing to 5000 samples, we get:</p>
<pre class="r"><code>mp5000 &lt;- monte_pi(samples = 5000)
as_ggplot(mp5000$p)</code></pre>
<p><img src="/post/2020-04-06-i-m-here-to-sample-the-pi_files/figure-html/unnamed-chunk-10-1.png" width="672" />
with <span class="math inline">\(P^{(5000)}=\)</span> 3.1528. That’s a relative error of the order of 0.3567409 percent. That’s not too bad, for a “dumb way to estimate <span class="math inline">\(\pi\)</span>”. Let’s see what happens when we use 50000 samples. We get:</p>
<pre class="r"><code>mp50000 &lt;- monte_pi(samples = 50000)
as_ggplot(mp50000$p)</code></pre>
<p><img src="/post/2020-04-06-i-m-here-to-sample-the-pi_files/figure-html/unnamed-chunk-11-1.png" width="672" />
with <span class="math inline">\(P^{(50000)}=\)</span> 3.15392. That’s a relative error of the order of 0.3923916 percent.</p>
</div>
<div id="capturing-the-uncertainty-in-our-estimator." class="section level2">
<h2>Capturing the uncertainty in our estimator.</h2>
<p>Let’s now construct a density plot of the distribution of the estimator <span class="math inline">\(P^{(n)}\)</span> for <span class="math inline">\(n=1500\)</span>, using <span class="math inline">\(N=2500\)</span> repeated runs of the <code>monte_pi()</code> function to get a feel for the uncertainty associated with the estimator.</p>
<pre class="r"><code>N &lt;- 2500
pi_estimate &lt;- rep(NA, times = N)
for(i in 1:N){
    s &lt;- runif(1, min = 1, max = 100000)
    mp &lt;- monte_pi(samples = 1500, seed = s)
    pi_estimate[i] &lt;- mp$df$prop[1500]
}</code></pre>
<p>The above generates the sample of estimates of pi. Yhe density plot of the output is as follows:</p>
<pre class="r"><code>dat &lt;- tibble(pi_estimate)
xlabel &lt;- &quot;Estimate of pi&quot;
ylabel &lt;- &quot;Density&quot;
maintitle &lt;- &quot;Density of expression(P^1500) based on 2500 runs&quot;

ggplot(dat, aes(x = pi_estimate)) +
    geom_density(aes(y = ..density..), fill = &quot;#4271AE&quot;, colour = &quot;#1F3552&quot;) +
    geom_rug() +
    geom_vline(xintercept = pi, colour = &quot;red&quot;, linetype = &quot;dashed&quot;, size = 1.2) +
    labs(x = xlabel, y = ylabel, title = maintitle)</code></pre>
<p><img src="/post/2020-04-06-i-m-here-to-sample-the-pi_files/figure-html/unnamed-chunk-13-1.png" width="672" />
This output is really interesting. It indicates that this naive monte carlo sampling method appears to concentrate probability mass around the <span class="math inline">\(3.14159\ldots\)</span> value (i.e. the “exact” value) of <span class="math inline">\(\pi\)</span>, but this method still leaves us with quite a degree of uncertainty about the true value of <span class="math inline">\(\pi\)</span>. A 95% central probability interval is given as:</p>
<pre class="r"><code>q &lt;- quantile(x = pi_estimate,
         probs = c(0.025, 0.975))
q</code></pre>
<pre><code>##     2.5%    97.5% 
## 3.069333 3.245333</code></pre>
<p>which indicates that if we didn’t know the value of pi, values between 3.0693333 and 3.2453333 are reasonably credible. Of course the true valu does lie within this region, and the mode of the distribution is
numeric. Similarly the mean is 3.1408117 and the median is 3.1413333. I suppose this is doing a reasonable job, but we could probably do far better too. Perhaps this is why Iain Murray calls this “a dumb approximation of <span class="math inline">\(\pi\)</span>!”</p>
</div>
